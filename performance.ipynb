{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class Timer:\n",
    "    def __init__(self, s, iters):\n",
    "        self.s = s\n",
    "        self.iters = iters\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        print(\"\\\"\" + self.s + \"\\\"\", \"took\", (time.perf_counter() - self.start)/self.iters*1e9, \"nanoseconds per db row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"db loading\" took 7517.942299949937 nanoseconds per db row\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from starter import compute_fingerprint, load_database\n",
    "\n",
    "with Timer(\"db loading\", 1000000):\n",
    "    fingerprints = load_database(\"database_fingerprints.npy\")\n",
    "    molecules = pd.read_csv(\"database.csv\")\n",
    "with open(\"query.txt\") as q:\n",
    "  query = q.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup\n",
    "Just python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"python coverage function\" took 4513920.100056566 nanoseconds per db row\n",
      "[0.2515592515592516, 0.22245322245322247, 0.2785862785862786, 0.22453222453222454, 0.30353430353430355]\n",
      "[928  66  59 429 428] [0.4261954261954262, 0.4303534303534304, 0.4365904365904366, 0.46361746361746364, 0.498960498960499]\n"
     ]
    }
   ],
   "source": [
    "def coverage(query, ref):\n",
    "  query_f = compute_fingerprint(query)\n",
    "  ref_f = compute_fingerprint(ref)\n",
    "  return sum(query_f & ref_f) / sum(query_f)\n",
    "\n",
    "with Timer(\"python coverage function\", 1000):\n",
    "    scores = [coverage(query, molecules[\"smiles\"][i]) for i in range(1000)]\n",
    "    topk = np.argsort(scores)[-k:]\n",
    "print(scores[:k])\n",
    "print(topk, [scores[i] for i in topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some sanity checks before we proceed\n",
    "After we check that these scores are meaningful, we can proceed and just verify that our scores are exactly identical to this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.746268656716418\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(coverage(molecules[\"smiles\"][0], molecules[\"smiles\"][0])) # should be 1\n",
    "print(coverage(molecules[\"smiles\"][0], molecules[\"smiles\"][1])) # should be between 0 and 1\n",
    "print(coverage(\"c1ccccc1\", \"C1CCCCC1\")) # should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed things up with NumPy\n",
    "We can get a massive speedup by using NumPy; everything is automatically vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy direct bitwise_and\" took 37394.42100049928 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[94115 39023 94179 94185 38955] [0.60914761 0.59459459 0.6029106  0.6008316  0.5966736 ]\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"numpy direct bitwise_and\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = np.sum(query_f & fingerprints[:100000], axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packing bits\n",
    "Packing bits allows us to `&` 8 bits at a time.\n",
    "\n",
    "Unpacking and summing is extremely inefficient though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy packed bits bitwise_and\" took 6823.157999897376 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[94115 39023 94179 94185 38955] [0.60914761 0.59459459 0.6029106  0.6008316  0.5966736 ]\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"numpy packed bits bitwise_and\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = np.sum(np.unpackbits(np.packbits(fingerprints[:100000], axis=1) & np.packbits(query_f)).reshape(-1, 2048), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython\n",
    "We need C++ to get even faster; I want to use the native popcount instruction because [NumPy doesn't have a function to do that yet](https://github.com/numpy/numpy/issues/16325)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/clive/anaconda3/envs/myenv/bin/cython\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 4.8.5\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /home/clive/code/reverie-challenge/reverie-challenge/popcount.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "In file included from \u001b[01m\u001b[K/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/numpy/core/include/numpy/ndarraytypes.h:1822:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Kpopcount.c:610\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
      " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
      "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kpopcount.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid __pyx_f_8popcount__fused_popcount64_bitwise_and_avx_topk_blosc(__Pyx_memviewslice, __Pyx_memviewslice, __Pyx_memviewslice, __Pyx_memviewslice, int)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kpopcount.c:5338:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
      "     if (unlikely(!((\u001b[01;35m\u001b[K__pyx_v_blocksize == __Pyx_div_long((__pyx_v_8popcount_BLOCK_SIZE_IN_ROWS * 0x800), 8)\u001b[m\u001b[K) != 0))) {\n",
      "                     \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kpopcount.c:810:43:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[Kunlikely\u001b[m\u001b[K’\n",
      "   #define unlikely(x) __builtin_expect(!!(\u001b[01;36m\u001b[Kx\u001b[m\u001b[K), 0)\n",
      "                                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "popcount.c:9967:5: note: basic block vectorized\n",
      "popcount.c:5693:5: note: basic block vectorized\n",
      "popcount.c:5741:17: note: basic block vectorized\n",
      "popcount.c:4997:5: note: basic block vectorized\n",
      "popcount.c:4566:5: note: basic block vectorized\n",
      "popcount.c:4577:15: note: basic block vectorized\n",
      "popcount.c:4215:5: note: basic block vectorized\n",
      "popcount.c:4226:15: note: basic block vectorized\n",
      "popcount.c:3674:5: note: basic block vectorized\n",
      "popcount.c:3685:15: note: basic block vectorized\n",
      "popcount.c:3338:5: note: basic block vectorized\n",
      "popcount.c:3349:15: note: basic block vectorized\n",
      "popcount.c:3044:5: note: basic block vectorized\n",
      "popcount.c:3055:15: note: basic block vectorized\n",
      "popcount.c:2844:5: note: basic block vectorized\n",
      "popcount.c:2855:15: note: basic block vectorized\n",
      "popcount.c:27606:17: note: basic block vectorized\n",
      "popcount.c:17209:3: note: basic block vectorized\n",
      "popcount.c:17595:33: note: loop vectorized\n",
      "popcount.c:17595:33: note: loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:17595:33: note: loop vectorized\n",
      "popcount.c:17595:33: note: loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:17595:33: note: loop vectorized\n",
      "popcount.c:17595:33: note: loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:17595:33: note: loop vectorized\n",
      "popcount.c:17595:33: note: loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:23501:32: note: basic block vectorized\n",
      "popcount.c:23363:22: note: basic block vectorized\n",
      "popcount.c:17595:33: note: loop vectorized\n",
      "popcount.c:17595:33: note: loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:17595:33: note: loop vectorized\n",
      "popcount.c:17595:33: note: loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:14991:50: note: basic block vectorized\n",
      "popcount.c:12215:0: note: loop vectorized\n",
      "popcount.c:12215:0: note: loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:12161:0: note: basic block vectorized\n",
      "popcount.c:12170:0: note: basic block vectorized\n",
      "popcount.c:22004:0: note: basic block vectorized\n",
      "popcount.c:11856:0: note: basic block vectorized\n",
      "popcount.c:11865:0: note: basic block vectorized\n",
      "popcount.c:8241:0: note: loop vectorized\n",
      "popcount.c:20197:0: note: basic block vectorized\n",
      "popcount.c:18387:0: note: loop vectorized\n",
      "popcount.c:18694:0: note: loop vectorized\n",
      "popcount.c:18387:0: note: loop vectorized\n",
      "popcount.c:18654:0: note: basic block vectorized\n",
      "popcount.c:26884:26: note: basic block vectorized\n",
      "popcount.c:27698:17: note: basic block vectorized\n",
      "popcount.c:27698:17: note: basic block vectorized\n",
      "popcount.c:27721:17: note: basic block vectorized\n",
      "popcount.c:3812:3: note: basic block vectorized\n",
      "popcount.c:27698:17: note: basic block vectorized\n",
      "popcount.c:27698:17: note: basic block vectorized\n",
      "popcount.c:27721:17: note: basic block vectorized\n",
      "popcount.c:4704:3: note: basic block vectorized\n",
      "popcount.c:27606:17: note: basic block vectorized\n",
      "popcount.c:27606:17: note: basic block vectorized\n",
      "popcount.c:22765:5: note: basic block vectorized\n",
      "popcount.c:27629:17: note: basic block vectorized\n",
      "popcount.c:26884:26: note: basic block vectorized\n",
      "popcount.c:27652:17: note: basic block vectorized\n",
      "popcount.c:27606:17: note: basic block vectorized\n",
      "popcount.c:3179:3: note: basic block vectorized\n",
      "popcount.c:27675:17: note: basic block vectorized\n",
      "popcount.c:26884:26: note: basic block vectorized\n",
      "popcount.c:27652:17: note: basic block vectorized\n",
      "popcount.c:27606:17: note: basic block vectorized\n",
      "popcount.c:3551:3: note: basic block vectorized\n",
      "popcount.c:27698:17: note: basic block vectorized\n",
      "popcount.c:27698:17: note: basic block vectorized\n",
      "popcount.c:27744:17: note: basic block vectorized\n",
      "popcount.c:5136:48: note: basic block vectorized\n",
      "popcount.c:27698:17: note: basic block vectorized\n",
      "popcount.c:27698:17: note: basic block vectorized\n",
      "popcount.c:27767:17: note: basic block vectorized\n",
      "popcount.c:27744:17: note: basic block vectorized\n",
      "popcount.c:5885:48: note: basic block vectorized\n",
      "popcount.c:27698:17: note: basic block vectorized\n",
      "popcount.c:27698:17: note: basic block vectorized\n",
      "popcount.c:27606:17: note: basic block vectorized\n",
      "popcount.c:4350:0: note: basic block vectorized\n"
     ]
    }
   ],
   "source": [
    "# Note: -O3 -march=native isn't just for fun, you really do need it to beat NumPy.\n",
    "! which cython || conda install -y cython\n",
    "! conda install -y python-blosc\n",
    "! cython popcount.pyx\n",
    "#! g++ -Wall -O3 -Ofast -g -flto -lm -L$$CONDA_PREFIX/lib -lblosc -shared -pthread -fPIC -funroll-loops -fno-strict-aliasing -march=native -mno-avx256-split-unaligned-load -fopt-info-vec-optimized -I$$CONDA_PREFIX/include/python3.6m -I$$CONDA_PREFIX/include -I$$CONDA_PREFIX/lib/python3.6/site-packages/numpy/core/include/ -o popcount.so popcount.c\n",
    "! g++ -Wall -O3 -Ofast -g -flto -fPIC -funroll-loops -fwrapv -fno-strict-aliasing -mavx2 -fopt-info-vec-optimized -I$CONDA_PREFIX/include/python3.6m -I$CONDA_PREFIX/include -I$CONDA_PREFIX/lib/python3.6/site-packages/numpy/core/include/ -mno-avx256-split-unaligned-load -c -o popcount.o -L$CONDA_PREFIX/lib popcount.c\n",
    "! g++ -Wall -O3 -Ofast -g -flto -shared -pthread -fPIC -funroll-loops -fwrapv -fno-strict-aliasing -mavx2 -fopt-info-vec-optimized -mno-avx256-split-unaligned-load -o popcount.so popcount.o -L$CONDA_PREFIX/lib -Wl,-rpath=$CONDA_PREFIX/lib -lblosc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy packed bits popcounted\" took 1217.8646000102162 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import inplace_popcount_32\n",
    "with Timer(\"numpy packed bits popcounted\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    isct = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1) & np.packbits(query_f), dtype=np.uint32)\n",
    "    inplace_popcount_32(isct)\n",
    "    scores = np.sum(isct.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're processing more than 5x faster than it takes to read the database off the SSD, we can declare victory, but also I'm curious how far we can go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing operations\n",
    "This is sort of a diminishing returns scenario; I needed `-O3 -march=native` to run the bitwise and faster than NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy fused bitwise_and + popcount\" took 854.0205999743193 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and\n",
    "with Timer(\"numpy fused bitwise_and + popcount\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "    fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing the packing too\n",
    "The packbits function is now the slowest part, at about 80% of the runtime. What if we don't do that at all?\n",
    "\n",
    "It ends up being slower than NumPy because NumPy is [using proper SIMD instructions for this](https://github.com/numpy/numpy/blob/97d2db483fc0ffd46f38d0e1c39d5fc001e33197/numpy/core/src/multiarray/compiled_base.c#L1543). This is okay, we now have the tools to do this properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"fused notpacked popcount\" took 1660.315899993293 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and_notpacked_count\n",
    "with Timer(\"fused notpacked popcount\", 1000000):\n",
    "    query_f = compute_fingerprint(query).astype(np.uint8)\n",
    "    fused_popcount_bitwise_and_notpacked_count(query_f, fingerprints[:1000000])\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endgame: AVX2\n",
    "AVX2 is supported by most modern processors, including the Intel processor that the specified MBP 2019 has.\n",
    "\n",
    "We get an absurdly high throughput: we're processing about 5 boolean database entries per nanosecond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"avx2\" took 202.34980003442615 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_avx2\n",
    "with Timer(\"avx2\", 1000000):\n",
    "    query_f = compute_fingerprint(query).astype(np.uint8)\n",
    "    scores = fused_popcount_avx2(query_f, fingerprints[:1000000]) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments and things to improve\n",
    "\n",
    "Algorithmic complexity is O(database size), since it must iterate over all the booleans in every row. I'm fairly certain that this cannot be asymptotically improved - for example in the worst case every molecule in the database contains all substructures (all 2048 bits are true).\n",
    "\n",
    "Memory usage is high; we're reading the entire database into memory and keeping it there. There isn't really any way around keeping O(database size) in memory because SSD bandwidth is far too low to support streaming from disk, but an 8x memory usage/bandwidth bottleneck improvement can probably be found by bitpacking the database beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Overtime stuff\n",
    "\n",
    "This was done for fun significantly over the 6hr time limit; please don't count this if you're evaluating quantity of code produced.\n",
    "\n",
    "## Pre-packing\n",
    "\n",
    "I did no memory-related optimizations mostly because I was more interested in playing with AVX, and because up until AVX the whole thing was compute-bottlenecked rather than memory-bottlenecked. However it's trivial to do so; just don't count the bitpacking step from the `Fusing operations` section, and you get (only!) a slight speedup over the AVX implementation as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 667.8555000107735 nanoseconds per db row\n",
      "\"pre-packed rows\" took 263.14210006967187 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "with Timer(\"pre-packed rows\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "    fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVX2 popcount attempt 1\n",
    "\n",
    "Not working correctly, but AVX2 emulated popcnt [can be 30% faster](https://stackoverflow.com/a/50082218) than the dedicated instruction.\n",
    "\n",
    "Edit: I think it won't be faster; according to [this paper](https://arxiv.org/pdf/1611.07612.pdf) and other sources, popcnt on 64-bit ints is the fastest way to do it at our data size of 2048 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 903.3192000351846 nanoseconds per db row\n",
      "\"pre-packed rows\" took 154.08700006082654 nanoseconds per db row\n",
      "[4.03833063e+15 2.62815452e+16 3.77945367e+15 3.34960946e+16\n",
      " 8.88579474e+15]\n",
      "[439306  28215  93292 489702 106223] [3.83505627e+16 3.83506231e+16 3.83505872e+16 3.83507498e+16\n",
      " 3.83504301e+16]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_avx2_emulated_popcount\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint64)\n",
    "    fused_avx2_emulated_popcount(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcount64 and some tweaks to get good assembly\n",
    "\n",
    "Let's use popcount64 directly on 64-bit ints, instead of 32-bit ints, and also do counting in the kernel itself. (Also added loop unrolling flag, for the slight benefit of all these examples.)\n",
    "\n",
    "Maybe let's be less guessy. Here's a relevant assembly sample from a single iteration of the unrolled loop.\n",
    "```\n",
    "   27c1a:       44 09 c2                or     %r8d,%edx\n",
    "   27c1d:       41 89 07                mov    %eax,(%r15)\n",
    "   27c20:       48 63 ca                movslq %edx,%rcx\n",
    "   27c23:       49 0f af cc             imul   %r12,%rcx\n",
    "   27c27:       49 8b 14 0b             mov    (%r11,%rcx,1),%rdx\n",
    "   27c2b:       48 23 17                and    (%rdi),%rdx\n",
    "   27c2e:       31 c9                   xor    %ecx,%ecx\n",
    "   27c30:       4c 01 cf                add    %r9,%rdi\n",
    "   27c33:       f3 48 0f b8 ca          popcnt %rdx,%rcx\n",
    "   27c38:       8d 56 02                lea    0x2(%rsi),%edx\n",
    "   27c3b:       01 c8                   add    %ecx,%eax\n",
    "```\n",
    "There might be a tiny bit of gain to be had by avoiding some of these instructions. I don't fully understand what they do, though.\n",
    "\n",
    "Making `count` as a separate int variable (instead of accumulating directly into `counts[i>>5]`) results in much less consistent loop unrolling - there doesn't actually seem to be a particular pattern to the iterations; they have a bunch of random instructions squeezed in between that don't necessarily repeat. Finally, making `fingerprints_packed_curr` as a temporary variable instead of calculating `i|j` every time makes the loop much tighter:\n",
    "```\n",
    "   27d17:       4c 8b 79 d0             mov    -0x30(%rcx),%r15    ; Get a quadword of fingerprints_packed_curr (fixed offset)\n",
    "   27d1b:       4c 23 3a                and    (%rdx),%r15         ; AND it with a query quadword\n",
    "   27d1e:       48 01 f2                add    %rsi,%rdx           ; Advance the query pointer\n",
    "   27d21:       f3 4d 0f b8 ff          popcnt %r15,%r15           ; Popcount it\n",
    "   27d26:       41 01 c7                add    %eax,%r15d          ; Add this to the count\n",
    "```\n",
    "Those two modifications combined give an additional 25-30% better performance. I don't think there's anything else that can be done to speed this up, unless AVX2 has some real magic to offer (but even then, the frequency hit from AVX/AVX2 probably means that it won't be able to overcome this).\n",
    "\n",
    "The fastest I've seen for just the `fused_popcount64_bitwise_and` part is 30ns:\n",
    "- 2048bits/30ns = 8.5 GBps, out of a memory bandwidth of ~10-20GBps.\n",
    "- 32 popcounts / 30ns = approx one 64-bit popcount per 0.9 nanoseconds. This almost double the expected CPI of 5 cycles @ 3GHz = 1.6 ns, assuming the 5 listed instructions above take 1 cycle each, but apparently this is very superscalar, or my math is wrong (is frequency == cycles per second?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 814.020499936305 nanoseconds per db row\n",
      "\"pre-packed rows\" took 41.421079996507615 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='little'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows\", 10000000):\n",
    "    for i in range(10):\n",
    "        query_f = compute_fingerprint(query)\n",
    "        query_packed = np.frombuffer(np.packbits(query_f, bitorder='little'), dtype=np.uint64)\n",
    "        counts = fused_popcount64_bitwise_and(query_packed, fingerprints_packed)\n",
    "        total = fused_popcount64_bitwise_and(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "        topk = np.argpartition(-counts, k)[:k]\n",
    "print(counts[:k]/total)\n",
    "print(topk, counts[topk]/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVX2 attempt 2: success and maximum throughput\n",
    "\n",
    "It might be possible to use AVX2 instructions to MOV or AND or ADD more efficiently; 10ns is the absolute limit because the popcount instruction has a CPI of 1. I'm highly doubtful it'll make things faster given that the width is only 256 bits anyway though, and there has to be some penalty moving between AVX registers and the regular registers where popcnt lives, though maybe that can be pipelined away.\n",
    "- LOADU twice for 256 bits each\n",
    "- AND 256 bits together\n",
    "- _mm256_extract_epi64 and popcount each of the 4 registers, adding\n",
    "\n",
    "With some guidance from [this](https://github.com/WojciechMula/sse-popcount/blob/master/popcnt-avx2-lookup.cpp) we can get a 20% speed boost.\n",
    "\n",
    "It turns out that even though benchmarks show unrolled loop of int64 popcnt is the fastest popcount implementation for 256 bytes, for our usecase we have additional math alongside it (the AND) so putting everything into AVX is actually beneficial.\n",
    "\n",
    "Our inner loop looks something like (\"something like\" because loop unrolling has made things confusing) this:\n",
    "```\n",
    "   28b23:       c4 43 25 38 6d 10 01    vinserti128 $0x1,0x10(%r13),%ymm11,%ymm13\n",
    "   28b2a:       c5 d5 71 d4 04          vpsrlw $0x4,%ymm4,%ymm5\n",
    "   28b2f:       c4 63 35 38 54 07 50    vinserti128 $0x1,0x50(%rdi,%rax,1),%ymm9,%ymm10\n",
    "   28b36:       01 \n",
    "   28b37:       c4 c1 7d fc c7          vpaddb %ymm15,%ymm0,%ymm0\n",
    "   28b3c:       c5 6d db c5             vpand  %ymm5,%ymm2,%ymm8\n",
    "   28b40:       c5 dd db f2             vpand  %ymm2,%ymm4,%ymm6\n",
    "   28b44:       c4 c1 15 db ca          vpand  %ymm10,%ymm13,%ymm1\n",
    "   28b49:       c4 42 1d 00 f0          vpshufb %ymm8,%ymm12,%ymm14\n",
    "   28b4e:       c4 41 7a 6f 14 24       vmovdqu (%r12),%xmm10\n",
    "   28b54:       c4 e2 1d 00 fe          vpshufb %ymm6,%ymm12,%ymm7\n",
    "   28b59:       c5 7a 6f 44 07 60       vmovdqu 0x60(%rdi,%rax,1),%xmm8\n",
    "```\n",
    "Even though it's longer, it's also handling 4x as much data per iteration.\n",
    "\n",
    "Turning on `-march=native -mno-avx256-split-unaligned-load` gets us to something like\n",
    "```\n",
    "   289ac:       c4 41 45 db cd          vpand  %ymm13,%ymm7,%ymm9\n",
    "   289b1:       c5 7d fc fe             vpaddb %ymm6,%ymm0,%ymm15\n",
    "   289b5:       c5 ed db bc 07 a0 00    vpand  0xa0(%rdi,%rax,1),%ymm2,%ymm7\n",
    "   289bc:       00 00 \n",
    "   289be:       c4 c1 05 fc f6          vpaddb %ymm14,%ymm15,%ymm6\n",
    "   289c3:       c4 e2 1d 00 e1          vpshufb %ymm1,%ymm12,%ymm4\n",
    "   289c8:       c4 c1 15 db c8          vpand  %ymm8,%ymm13,%ymm1\n",
    "   289cd:       c4 c2 1d 00 d9          vpshufb %ymm9,%ymm12,%ymm3\n",
    "   289d2:       c5 bd 71 d7 04          vpsrlw $0x4,%ymm7,%ymm8\n",
    "```\n",
    "Where we only see instructions that we've specified explicitly in the code. In fact, we're missing a bunch of `vmovdqu`'s - the compiler is doing something clever here that I don't understand.\n",
    "\n",
    "- 2048bits/22ns = 11.6 GBps, out of a memory bandwidth of ~10-20GBps.\n",
    "- ~80 AVX instructions / 22ns = approx 3.6GHz, which is significantly higher than the clock frequency. This is probably because some instructions can have a throughput of more than 1 per cycle.\n",
    "\n",
    "This may well be the end of the line for optimizing this, as this does seem to be optimal (all sources point to Mula as the source of these algorithms, and this is the best his papers have to offer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 773.4832999994978 nanoseconds per db row\n",
      "\"pre-packed rows 1\" took 1.4030399965122342 nanoseconds per db row\n",
      "\"pre-packed rows 2\" took 0.04510000580921769 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 32.14646000415087 nanoseconds per db row\n",
      "\"pre-packed rows 4\" took 0.007049995474517345 nanoseconds per db row\n",
      "\"pre-packed rows 5\" took 7.317620003595948 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='little'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 1\", 10000000):\n",
    "    for i in range(10):\n",
    "        query_f = compute_fingerprint(query)\n",
    "with Timer(\"pre-packed rows 2\", 10000000):\n",
    "    for i in range(10):\n",
    "        query_packed = np.frombuffer(np.packbits(query_f, bitorder='little'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 3\", 10000000):\n",
    "    for i in range(10):\n",
    "        counts = fused_popcount64_bitwise_and_avx(query_packed, fingerprints_packed)\n",
    "with Timer(\"pre-packed rows 4\", 10000000):\n",
    "    for i in range(10):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "with Timer(\"pre-packed rows 5\", 10000000):\n",
    "    for i in range(10):\n",
    "        topk = np.argpartition(-counts, k)[:k]\n",
    "print(counts[:k]/total)\n",
    "print(topk, counts[topk]/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finale: Optimizing other parts of the system\n",
    "\n",
    "We're actually getting to the point where the final np.argpartition on 1 million elements is taking about 25% of the total time. Using np.argsort is 10x slower. We did actually need this initially seemingly unnecessary optimization!\n",
    "\n",
    "There are [ways](https://github.com/WojciechMula/simd-sort) to make this happen faster, but since we're going to be generating the entire count array anyway, why not just construct it on the fly? Specifically, let's use a **min-heap of size k** to track the k largest elements we've seen.\n",
    "\n",
    "This absorbs the topk algorithm without any measurable performance penalty (less than 10% with k=1000). A bit of tweaking suffices to double the speed of the compute_fingerprint, and now we are truly at the end of the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 805.2884000353515 nanoseconds per db row\n",
      "\"pre-packed rows 0\" took 0.28430999955162406 nanoseconds per db row\n",
      "\"pre-packed rows 1\" took 0.5352600011974573 nanoseconds per db row\n",
      "\"pre-packed rows 2\" took 0.03544000210240483 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 25.93242999864742 nanoseconds per db row\n",
      "\"pre-packed rows 4\" took 0.004920002538710833 nanoseconds per db row\n",
      "[324758 329123 324807 324852 329172] [0.66735967 0.66735967 0.66735967 0.66943867 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx_topk, fused_popcount64_bitwise_and_avx\n",
    "from rdkit import Chem\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='big'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 0\", 10000000):\n",
    "    for i in range(10):\n",
    "        mol = Chem.MolFromSmiles(query)\n",
    "with Timer(\"pre-packed rows 1\", 10000000):\n",
    "    for i in range(10):\n",
    "        s = Chem.RDKFingerprint(mol, fpSize=2048, maxPath=5).ToBitString()\n",
    "with Timer(\"pre-packed rows 2\", 10000000):\n",
    "    for i in range(10):\n",
    "        query_packed = np.copy(np.frombuffer(int(s, 2).to_bytes(len(s) // 8, byteorder='big'), dtype=np.uint64))\n",
    "with Timer(\"pre-packed rows 3\", 10000000):\n",
    "    for i in range(10):\n",
    "        counts, topk = fused_popcount64_bitwise_and_avx_topk(query_packed, fingerprints_packed, k)\n",
    "with Timer(\"pre-packed rows 4\", 10000000):\n",
    "    for i in range(10):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "print(topk, counts/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed: blosc\n",
    "Since we're at 10GBps, we might have a slight memory bottleneck, does blosc help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression: 0.70317598828125\n"
     ]
    }
   ],
   "source": [
    "import blosc\n",
    "fingerprints_blosc_compressed = np.frombuffer(blosc.compress(fingerprints_packed, typesize=8, shuffle=blosc.NOSHUFFLE, cname='blosclz'), dtype=np.uint8)\n",
    "\n",
    "print(\"Compression:\", len(fingerprints_blosc_compressed)/8/len(fingerprints_packed))\n",
    "# %timeit blosc.decompress(fingerprints_blosc_compressed)\n",
    "# %timeit np.copy(fingerprints_packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 2476.6889000311494 nanoseconds per db row\n",
      "\"pre-packed rows 0\" took 0.8969000773504376 nanoseconds per db row\n",
      "\"pre-packed rows 1\" took 0.8905000286176801 nanoseconds per db row\n",
      "\"pre-packed rows 2\" took 0.3274999326094985 nanoseconds per db row\n",
      "('INFO:', 256000000, 180013053, 1048576)\n",
      "\"pre-packed rows 3\" took 830.3715999936685 nanoseconds per db row\n",
      "\"pre-packed rows 4\" took 0.037100049667060375 nanoseconds per db row\n",
      "[324758 329123 324807 324852 329172] [0.66735967 0.66735967 0.66735967 0.66943867 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx_topk_blosc, fused_popcount64_bitwise_and_avx\n",
    "from rdkit import Chem\n",
    "import blosc\n",
    "\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints, axis=1, bitorder='big'), dtype=np.uint64)\n",
    "    fingerprints_blosc_compressed = np.frombuffer(blosc.compress(fingerprints_packed, typesize=8, shuffle=blosc.NOSHUFFLE, cname='blosclz'), dtype=np.uint8)\n",
    "    n_fingerprints = len(fingerprints)\n",
    "with Timer(\"pre-packed rows 0\", 1000000):\n",
    "    for i in range(1):\n",
    "        mol = Chem.MolFromSmiles(query)\n",
    "with Timer(\"pre-packed rows 1\", 1000000):\n",
    "    for i in range(1):\n",
    "        s = Chem.RDKFingerprint(mol, fpSize=2048, maxPath=5).ToBitString()\n",
    "with Timer(\"pre-packed rows 2\", 1000000):\n",
    "    for i in range(1):\n",
    "        query_packed = np.copy(np.frombuffer(int(s, 2).to_bytes(len(s) // 8, byteorder='big'), dtype=np.uint64))\n",
    "with Timer(\"pre-packed rows 3\", 1000000):\n",
    "    for i in range(1):\n",
    "        counts, topk = fused_popcount64_bitwise_and_avx_topk_blosc(query_packed, fingerprints_blosc_compressed, k, n_fingerprints)\n",
    "with Timer(\"pre-packed rows 4\", 1000000):\n",
    "    for i in range(1):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "print(topk, counts/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, that was a dramatic failure. Maybe next step is to try out custom encodings like RLE or even sparse bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future stuff\n",
    "AVX-512 would be fun but I don't have ice lake hardware sadly. This should be something like 3x faster, less any AVX-induced processor frequency hit.\n",
    "- `_mm512_loadu_epi64`\n",
    "- `_mm512_and_epi64`\n",
    "- `_mm512_popcnt_epi64`\n",
    "- `_mm512_reduce_add_epi64`\n",
    "\n",
    "Some additional cleverness that I do not have time to explore, but which is unlikely to yield better results:\n",
    "- Database rows are almost always sparse. There might be more efficient popcount algorithms in this case.\n",
    "- OpenCL was originally planned but no time, and also I think my laptop iGPU doesn't support it anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Threading\n",
    "Because why not? The AVX2 code is mostly in C-land away from the GIL so we might be able to get close to linear speedup in the number of cores (regular cores; hyperthreading probably does not help for this because AVX2 resources are shared).\n",
    "\n",
    "This didn't help at all with the AVX2 code because there are other bottlenecks e.g. system memory bandwidth. 2GB dataset in 0.2s is 10GBps, which is around half the theoretical memory bandwidth my machine could have, but there's probably some other caveats there.\n",
    "\n",
    "This does help a lot (+50% ish) with regular popcount64 because of the 8x lower memory bandwidth requirement when bits are packed. 2GB/8 = 256MB dataset in 0.08s is only 3GBps, so it's probably still compute limited and could benefit from more threading. Unfortunately I only have 2 actual cores (4 hyperthreaded) so there can be no more than a 2x speedup.\n",
    "\n",
    "However modifying popcount64 to fuse the counting step as well results in having no significant threading benefit. Occasionally it will run faster than single-threaded, but most of the time it's running into some issue or another - maybe memory bandwidth, maybe scheduler issues because it's so fast.\n",
    "\n",
    "This also points toward there being very little point in using OpenCL: unless you have a discrete GPU with dedicated high bandwidth video memory, which isn't the case on MBP 2019, it's not going to get you much further, and more likely will slow things down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 872.9486999800429 nanoseconds per db row\n",
      "Using 2 threads\n",
      "\"threaded popcount64\" took 103.17909996956587 nanoseconds per db row\n",
      "[324852 324807 329123 324758 329172] [0.66943867 0.66735967 0.66735967 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from popcount import fused_popcount64_bitwise_and\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='little'), dtype=np.uint64)\n",
    "\n",
    "core_ids = set()\n",
    "with open('/proc/cpuinfo') as f:\n",
    "    for line in f:\n",
    "        if line.count('core id'):\n",
    "            core_ids.add(line)\n",
    "\n",
    "nthreads = len(core_ids)\n",
    "assert(len(fingerprints) % nthreads == 0) # Note: below code calculating start and end assumes that nthreads divides len(fingerprints)\n",
    "print(f\"Using {nthreads} threads\")\n",
    "\n",
    "topk_per_thread = [None] * nthreads\n",
    "topk_per_thread_scores = [None] * nthreads\n",
    "\n",
    "def thread_func(query_packed, threadid):\n",
    "    start = threadid*len(fingerprints_packed)//nthreads\n",
    "    end = (threadid+1)*len(fingerprints_packed)//nthreads\n",
    "    \n",
    "    counts = fused_popcount64_bitwise_and(query_packed, fingerprints_packed[start:end])\n",
    "    total = fused_popcount64_bitwise_and(query_packed, query_packed)\n",
    "\n",
    "    topk_per_thread[threadid] = np.argpartition(-counts, k)[:k]\n",
    "\n",
    "    topk_per_thread_scores[threadid] = counts[topk_per_thread[threadid]] / total\n",
    "    topk_per_thread[threadid] += start // 32\n",
    "\n",
    "with Timer(\"threaded popcount64\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f, bitorder='little'), dtype=np.uint64)\n",
    "    threads = [None] * nthreads\n",
    "    for i in range(1, nthreads):\n",
    "        threads[i] = threading.Thread(target=thread_func, args=(query_packed, i))\n",
    "        threads[i].start()\n",
    "    thread_func(query_packed, 0)\n",
    "    for i in range(1, nthreads):\n",
    "        threads[i].join()\n",
    "    topk_arg = np.argsort(-np.concatenate(topk_per_thread_scores))[:k]\n",
    "    scores = np.concatenate(topk_per_thread_scores)[topk_arg]\n",
    "    topk = np.concatenate(topk_per_thread)[topk_arg]\n",
    "print(topk, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: A failed attempt at using Numba\n",
    "Cython has some ... interoperability issues ... with Numba, so using the popcount kernel isn't possible.\n",
    "\n",
    "Using purely Numba (without the popcount kernel) is way slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from numba import jit\n",
    "except ImportError:\n",
    "    ! conda install -y numba\n",
    "    from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numba\" took 104080.5069997441 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[94115 39023 94179 94185 38955] [0.60914761 0.59459459 0.6029106  0.6008316  0.5966736 ]\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "# from numba.extending import get_cython_function_address\n",
    "# import numpy.ctypeslib as npct\n",
    "# import ctypes\n",
    "\n",
    "# array_1d_uint32 = npct.ndpointer(dtype=np.uint32, ndim=1, flags='CONTIGUOUS')\n",
    "# addr = get_cython_function_address(\"popcount\", \"_fused_popcount_bitwise_and\")\n",
    "# functype = ctypes.CFUNCTYPE(None, array_1d_uint32, array_1d_uint32)\n",
    "# fused_popcount_bitwise_and = functype(addr)\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def numba_func(query_f, fingerprints):\n",
    "    return np.sum(query_f & fingerprints[:100000], axis=1) / np.sum(query_f)\n",
    "\n",
    "# @numba.jit(nopython=True)\n",
    "# def numba_func(query_f, fingerprints):\n",
    "#     query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "#     fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "#     fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "#     return np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "\n",
    "with Timer(\"numba\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = numba_func(query_f, fingerprints)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import popcount\n",
    "popcount = importlib.reload(popcount)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
