{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class Timer:\n",
    "    def __init__(self, s, iters):\n",
    "        self.s = s\n",
    "        self.iters = iters\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        print(\"\\\"\" + self.s + \"\\\"\", \"took\", (time.perf_counter() - self.start)/self.iters*1000000, \"microsec per db row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"db loading\" took 11.724437099997886 microsec per db row\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from starter import compute_fingerprint, load_database\n",
    "\n",
    "with Timer(\"db loading\", 1000000):\n",
    "    fingerprints = load_database(\"database_fingerprints.npy\")\n",
    "    molecules = pd.read_csv(\"database.csv\")\n",
    "with open(\"query.txt\") as q:\n",
    "  query = q.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup\n",
    "Just python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"python coverage function\" took 3642.384099890478 microsec per db row\n",
      "[0.2515592515592516, 0.22245322245322247, 0.2785862785862786, 0.22453222453222454, 0.30353430353430355]\n",
      "[928  66  59 429 428] [0.4261954261954262, 0.4303534303534304, 0.4365904365904366, 0.46361746361746364, 0.498960498960499]\n"
     ]
    }
   ],
   "source": [
    "def coverage(query, ref):\n",
    "  query_f = compute_fingerprint(query)\n",
    "  ref_f = compute_fingerprint(ref)\n",
    "  return sum(query_f & ref_f) / sum(query_f)\n",
    "\n",
    "with Timer(\"python coverage function\", 1000):\n",
    "    scores = [coverage(query, molecules[\"smiles\"][i]) for i in range(1000)]\n",
    "    topk = np.argsort(scores)[-k:]\n",
    "print(scores[:k])\n",
    "print(topk, [scores[i] for i in topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some sanity checks before we proceed\n",
    "After we check that these scores are meaningful, we can proceed and just verify that our scores are exactly identical to this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.746268656716418\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(coverage(molecules[\"smiles\"][0], molecules[\"smiles\"][0])) # should be 1\n",
    "print(coverage(molecules[\"smiles\"][0], molecules[\"smiles\"][1])) # should be between 0 and 1\n",
    "print(coverage(\"c1ccccc1\", \"C1CCCCC1\")) # should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed things up with NumPy\n",
    "We can get a massive speedup by using NumPy; everything is automatically vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy direct bitwise_and\" took 58.05612300056964 microsec per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[94115 39023 94179 94185 38955] [0.60914761 0.59459459 0.6029106  0.6008316  0.5966736 ]\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"numpy direct bitwise_and\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = np.sum(query_f & fingerprints[:100000], axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packing bits\n",
    "Packing bits allows us to `&` 8 bits at a time.\n",
    "\n",
    "Unpacking and summing is extremely inefficient though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy packed bits bitwise_and\" took 12.280519999330863 microsec per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[94115 39023 94179 94185 38955] [0.60914761 0.59459459 0.6029106  0.6008316  0.5966736 ]\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"numpy packed bits bitwise_and\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = np.sum(np.unpackbits(np.packbits(fingerprints[:100000], axis=1) & np.packbits(query_f)).reshape(-1, 2048), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython\n",
    "We need C++ to get even faster; I want to use the native popcount instruction because [NumPy doesn't have a function to do that yet](https://github.com/numpy/numpy/issues/16325)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/clive/anaconda3/envs/myenv/bin/cython\n",
      "/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /home/clive/code/reverie-challenge/reverie-challenge/popcount.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "In file included from \u001b[01m\u001b[K/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/numpy/core/include/numpy/ndarraytypes.h:1822:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Kpopcount.c:610\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
      " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
      "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
      "popcount.c:20872:24: note: basic block vectorized\n",
      "popcount.c:24791:11: note: basic block vectorized\n",
      "popcount.c:20222:6: note: basic block vectorized\n",
      "popcount.c:24387:9: note: basic block vectorized\n",
      "popcount.c:20872:24: note: basic block vectorized\n",
      "popcount.c:25110:12: note: basic block vectorized\n",
      "popcount.c:10487:3: note: loop vectorized\n",
      "popcount.c:10487:3: note: loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:10433:97: note: basic block vectorized\n",
      "popcount.c:10442:98: note: basic block vectorized\n",
      "popcount.c:8239:8: note: basic block vectorized\n",
      "popcount.c:25110:12: note: basic block vectorized\n",
      "popcount.c:25110:12: note: basic block vectorized\n",
      "popcount.c:20938:8: note: basic block vectorized\n",
      "popcount.c:2743:8: note: basic block vectorized\n",
      "popcount.c:2754:15: note: basic block vectorized\n",
      "popcount.c:15481:6: note: basic block vectorized\n",
      "popcount.c:25133:12: note: basic block vectorized\n",
      "popcount.c:24387:9: note: basic block vectorized\n",
      "popcount.c:25156:12: note: basic block vectorized\n",
      "popcount.c:3078:3: note: basic block vectorized\n",
      "popcount.c:2943:8: note: basic block vectorized\n",
      "popcount.c:2954:15: note: basic block vectorized\n",
      "popcount.c:25202:12: note: basic block vectorized\n",
      "popcount.c:25202:12: note: basic block vectorized\n",
      "popcount.c:3668:3: note: basic block vectorized\n",
      "popcount.c:3533:8: note: basic block vectorized\n",
      "popcount.c:3544:15: note: basic block vectorized\n",
      "popcount.c:25110:12: note: basic block vectorized\n",
      "popcount.c:21887:32: note: basic block vectorized\n",
      "popcount.c:21513:22: note: basic block vectorized\n",
      "popcount.c:15867:3: note: loop vectorized\n",
      "popcount.c:15867:3: note: loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:15867:3: note: loop vectorized\n",
      "popcount.c:15867:3: note: loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:16659:3: note: loop vectorized\n",
      "popcount.c:16966:3: note: loop vectorized\n",
      "popcount.c:16659:3: note: loop vectorized\n",
      "popcount.c:16926:3: note: basic block vectorized\n",
      "popcount.c:23592:5: note: basic block vectorized\n",
      "popcount.c:23590:18: note: basic block vectorized\n",
      "popcount.c:23590:18: note: basic block vectorized\n",
      "popcount.c:15867:3: note: loop vectorized\n",
      "popcount.c:15867:3: note: loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:15867:3: note: loop vectorized\n",
      "popcount.c:15867:3: note: loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:15867:3: note: loop vectorized\n",
      "popcount.c:15867:3: note: loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:15867:3: note: loop vectorized\n",
      "popcount.c:15867:3: note: loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:13263:50: note: basic block vectorized\n",
      "popcount.c:6513:7: note: loop vectorized\n",
      "popcount.c:25179:12: note: basic block vectorized\n",
      "popcount.c:24387:9: note: basic block vectorized\n",
      "popcount.c:25152:12: note: basic block vectorized\n",
      "popcount.c:25156:12: note: basic block vectorized\n",
      "popcount.c:3404:107: note: basic block vectorized\n",
      "popcount.c:3450:3: note: basic block vectorized\n",
      "popcount.c:3237:8: note: basic block vectorized\n",
      "popcount.c:3248:15: note: basic block vectorized\n",
      "popcount.c:18469:8: note: basic block vectorized\n",
      "popcount.c:10128:99: note: basic block vectorized\n",
      "popcount.c:10137:100: note: basic block vectorized\n",
      "popcount.c:25202:12: note: basic block vectorized\n",
      "popcount.c:25202:12: note: basic block vectorized\n",
      "popcount.c:4203:3: note: basic block vectorized\n",
      "popcount.c:4071:8: note: basic block vectorized\n",
      "popcount.c:4082:15: note: basic block vectorized\n"
     ]
    }
   ],
   "source": [
    "# Note: -O3 -march=native isn't just for fun, you really do need it to beat NumPy.\n",
    "! which cython || conda install -y cython\n",
    "! cython popcount.pyx\n",
    "! gcc -Wall -O3 -g -lm -shared -pthread -fPIC -fwrapv -fno-strict-aliasing -mavx2 -fopt-info-vec-optimized -I$$CONDA_PREFIX/include/python3.6m -I$$CONDA_PREFIX/include -I$$CONDA_PREFIX/lib/python3.6/site-packages/numpy/core/include/ -o popcount.so popcount.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy packed bits popcounted\" took 1.7242891000350937 microsec per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import inplace_popcount_32\n",
    "with Timer(\"numpy packed bits popcounted\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    isct = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1) & np.packbits(query_f), dtype=np.uint32)\n",
    "    inplace_popcount_32(isct)\n",
    "    scores = np.sum(isct.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're processing more than 5x faster than it takes to read the database off the SSD, we can declare victory, but also I'm curious how far we can go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing operations\n",
    "This is sort of a diminishing returns scenario; I needed `-O3 -march=native` to run the bitwise and faster than NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy fused bitwise_and + popcount\" took 1.1449584000511095 microsec per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and\n",
    "with Timer(\"numpy fused bitwise_and + popcount\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "    fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing the packing too\n",
    "The packbits function is now the slowest part, at about 80% of the runtime. What if we don't do that at all?\n",
    "\n",
    "It ends up being slower than NumPy because NumPy is [using proper SIMD instructions for this](https://github.com/numpy/numpy/blob/97d2db483fc0ffd46f38d0e1c39d5fc001e33197/numpy/core/src/multiarray/compiled_base.c#L1543). This is okay, we now have the tools to do this properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"fused notpacked popcount\" took 2.180406400002539 microsec per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and_notpacked_count\n",
    "with Timer(\"fused notpacked popcount\", 1000000):\n",
    "    query_f = compute_fingerprint(query).astype(np.uint8)\n",
    "    fused_popcount_bitwise_and_notpacked_count(query_f, fingerprints[:1000000])\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endgame: AVX2\n",
    "AVX2 is supported by most modern processors, including the Intel processor that the specified MBP 2019 has.\n",
    "\n",
    "We get an absurdly high throughput: we're processing about 5 boolean database entries per nanosecond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"avx2\" took 0.22520780004560947 microsec per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_avx2\n",
    "with Timer(\"avx2\", 1000000):\n",
    "    query_f = compute_fingerprint(query).astype(np.uint8)\n",
    "    scores = fused_popcount_avx2(query_f, fingerprints[:1000000]) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments and things to improve\n",
    "\n",
    "Algorithmic complexity is O(database size), since it must iterate over all the booleans in every row. I'm fairly certain that this cannot be asymptotically improved - for example in the worst case every molecule in the database contains all substructures (all 2048 bits are true).\n",
    "\n",
    "Memory usage is high; we're reading the entire database into memory and keeping it there. There isn't really any way around keeping O(database size) in memory because SSD bandwidth is far too low to support streaming from disk, but an 8x memory usage/bandwidth bottleneck improvement can probably be found by bitpacking the database beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Overtime stuff\n",
    "\n",
    "This was done for fun significantly over the 6hr time limit; please don't count this if you're evaluating quantity of code produced.\n",
    "\n",
    "I did no memory-related optimizations mostly because I was more interested in playing with AVX, and because up until AVX the whole thing was compute-bottlenecked rather than memory-bottlenecked. However it's trivial to do so; just don't count the bitpacking step from the `Fusing operations` section, and you get (only!) a slight speedup over the AVX implementation as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 0.6740187000250444 microsec per db row\n",
      "\"pre-packed rows\" took 0.17853230005130172 microsec per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "with Timer(\"pre-packed rows\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "    fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfinished attempt at using popcount64 directly on 64-bit ints, instead of looping `__builtin_popcount` on 32-bit ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 0.6325727000366896 microsec per db row\n",
      "\"pre-packed rows\" took 0.1641679999884218 microsec per db row\n",
      "[2.44224911e+16 9.03778830e+15 2.53200250e+16 1.19146760e+16\n",
      " 5.83236742e+15]\n",
      "[ 24859 249809  98695 313458 310658] [3.83504309e+16 3.83507062e+16 3.83503865e+16 3.83503088e+16\n",
      " 3.83501115e+16]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='little'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f, bitorder='little'), dtype=np.uint64)\n",
    "    fused_popcount64_bitwise_and(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also not working correctly, but AVX2 emulated popcnt [can be 30% faster](https://stackoverflow.com/a/50082218) than the dedicated instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 0.7992424999829382 microsec per db row\n",
      "\"pre-packed rows\" took 0.16936449997592717 microsec per db row\n",
      "[4.03833063e+15 2.62815452e+16 3.77945367e+15 3.34960946e+16\n",
      " 8.88579474e+15]\n",
      "[439306  28215  93292 489702 106223] [3.83505627e+16 3.83506231e+16 3.83505872e+16 3.83507498e+16\n",
      " 3.83504301e+16]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_avx2_emulated_popcount\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint64)\n",
    "    fused_avx2_emulated_popcount(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional cleverness that I do not have time to explore:\n",
    "- A slight (2x?) additional memory usage improvement could be made by `blosc`-compressing the bitpacked database in-memory, though this is unlikely to change the overall runtime.\n",
    "- AVX-512 would be fun but I don't have ice lake hardware sadly\n",
    "- OpenCL was originally planned but no time, and also I think my laptop iGPU doesn't support it anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Threading\n",
    "Because why not? The AVX2 code is mostly in C-land away from the GIL so we might be able to get close to linear speedup in the number of cores (regular cores; hyperthreading probably does not help for this because AVX2 resources are shared).\n",
    "\n",
    "In practice this doesn't actually help at all, probably because the AVX2 code is so fast that there are other bottlenecks e.g. system memory bandwidth. 2GB dataset in 0.2s is 10GBps, which is somewhat over half the theoretical memory bandwidth my machine should have.\n",
    "\n",
    "This also points toward there being very little point in using OpenCL: unless you have a discrete GPU with dedicated high bandwidth video memory, which isn't the case on MBP 2019, it's not going to get you much further, and more likely will slow things down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"threaded avx2\" took 0.35625459998846054 microsec per db row\n",
      "[324807 324852 324758 329172 329123] [0.66735967 0.66943867 0.66735967 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from popcount import fused_popcount_bitwise_and\n",
    "\n",
    "nthreads = 4\n",
    "topk_per_thread = [None] * nthreads\n",
    "topk_per_thread_scores = [None] * nthreads\n",
    "\n",
    "def thread_func(query_f, threadid):\n",
    "    start = threadid*len(fingerprints)//nthreads\n",
    "    end = (threadid+1)*len(fingerprints)//nthreads # Assumes that nranks divides len(fingerprints)\n",
    "    scores = fused_popcount_avx2(query_f, fingerprints[start:end])\n",
    "\n",
    "    topk_per_thread[threadid] = np.argpartition(-scores, k)[:k]\n",
    "    topk_per_thread_scores[threadid] = scores[topk_per_thread[threadid]] / np.sum(query_f)\n",
    "    topk_per_thread[threadid] += start\n",
    "\n",
    "with Timer(\"threaded avx2\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "    threads = [None] * nthreads\n",
    "    for i in range(nthreads):\n",
    "        threads[i] = threading.Thread(target=thread_func, args=(query_f, i))\n",
    "        threads[i].start()\n",
    "    for i in range(nthreads):\n",
    "        threads[i].join()\n",
    "    topk_arg = np.argpartition(-np.concatenate(topk_per_thread_scores), k)[:k]\n",
    "    scores = np.concatenate(topk_per_thread_scores)[topk_arg]\n",
    "    topk = np.concatenate(topk_per_thread)[topk_arg]\n",
    "print(topk, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: A failed attempt at using Numba\n",
    "Cython has some ... interoperability issues ... with Numba, so using the popcount kernel isn't possible.\n",
    "\n",
    "Using purely Numba (without the popcount kernel) is way slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from numba import jit\n",
    "except ImportError:\n",
    "    ! conda install -y numba\n",
    "    from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numba\" took 105.78900799970143 microsec per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[94115 39023 94179 94185 38955] [0.60914761 0.59459459 0.6029106  0.6008316  0.5966736 ]\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "# from numba.extending import get_cython_function_address\n",
    "# import numpy.ctypeslib as npct\n",
    "# import ctypes\n",
    "\n",
    "# array_1d_uint32 = npct.ndpointer(dtype=np.uint32, ndim=1, flags='CONTIGUOUS')\n",
    "# addr = get_cython_function_address(\"popcount\", \"_fused_popcount_bitwise_and\")\n",
    "# functype = ctypes.CFUNCTYPE(None, array_1d_uint32, array_1d_uint32)\n",
    "# fused_popcount_bitwise_and = functype(addr)\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def numba_func(query_f, fingerprints):\n",
    "    return np.sum(query_f & fingerprints[:100000], axis=1) / np.sum(query_f)\n",
    "\n",
    "# @numba.jit(nopython=True)\n",
    "# def numba_func(query_f, fingerprints):\n",
    "#     query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "#     fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "#     fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "#     return np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "\n",
    "with Timer(\"numba\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = numba_func(query_f, fingerprints)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
