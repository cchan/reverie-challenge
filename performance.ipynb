{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class Timer:\n",
    "    def __init__(self, s, iters):\n",
    "        self.s = s\n",
    "        self.iters = iters\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        print(\"\\\"\" + self.s + \"\\\"\", \"took\", (time.perf_counter() - self.start)/self.iters*1e9, \"nanoseconds per db row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"db loading\" took 2373.546700022416 nanoseconds per db row\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from starter import compute_fingerprint, load_database\n",
    "\n",
    "with Timer(\"db loading\", 1000000):\n",
    "    fingerprints = load_database(\"database_fingerprints.npy\")\n",
    "    molecules = pd.read_csv(\"database.csv\")\n",
    "with open(\"query.txt\") as q:\n",
    "  query = q.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup\n",
    "Just python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"python coverage function\" took 2317983.599990839 nanoseconds per db row\n",
      "[0.2515592515592516, 0.22245322245322247, 0.2785862785862786, 0.22453222453222454, 0.30353430353430355]\n",
      "[928  66  59 429 428] [0.4261954261954262, 0.4303534303534304, 0.4365904365904366, 0.46361746361746364, 0.498960498960499]\n"
     ]
    }
   ],
   "source": [
    "def coverage(query, ref):\n",
    "  query_f = compute_fingerprint(query)\n",
    "  ref_f = compute_fingerprint(ref)\n",
    "  return sum(query_f & ref_f) / sum(query_f)\n",
    "\n",
    "with Timer(\"python coverage function\", 1000):\n",
    "    scores = [coverage(query, molecules[\"smiles\"][i]) for i in range(1000)]\n",
    "    topk = np.argsort(scores)[-k:]\n",
    "print(scores[:k])\n",
    "print(topk, [scores[i] for i in topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some sanity checks before we proceed\n",
    "After we check that these scores are meaningful, we can proceed and just verify that our scores are exactly identical to this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.746268656716418\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(coverage(molecules[\"smiles\"][0], molecules[\"smiles\"][0])) # should be 1\n",
    "print(coverage(molecules[\"smiles\"][0], molecules[\"smiles\"][1])) # should be between 0 and 1\n",
    "print(coverage(\"c1ccccc1\", \"C1CCCCC1\")) # should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed things up with NumPy\n",
    "We can get a massive speedup by using NumPy; everything is automatically vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy direct bitwise_and\" took 6664.069000107702 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[94115 39023 94179 94185 38955] [0.60914761 0.59459459 0.6029106  0.6008316  0.5966736 ]\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"numpy direct bitwise_and\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = np.sum(query_f & fingerprints[:100000], axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packing bits\n",
    "Packing bits allows us to `&` 8 bits at a time.\n",
    "\n",
    "Unpacking and summing is extremely inefficient though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy packed bits bitwise_and\" took 2031.6820000880396 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[94115 39023 94179 94185 38955] [0.60914761 0.59459459 0.6029106  0.6008316  0.5966736 ]\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"numpy packed bits bitwise_and\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = np.sum(np.unpackbits(np.packbits(fingerprints[:100000], axis=1) & np.packbits(query_f)).reshape(-1, 2048), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython\n",
    "We need C++ to get even faster; I want to use the native popcount instruction because [NumPy doesn't have a function to do that yet](https://github.com/numpy/numpy/issues/16325)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/clive/anaconda3/envs/my-rdkit-env/bin/cython\n",
      "/home/clive/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /home/clive/code/reverie-challenge/popcount.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\n",
      "Copyright (C) 2019 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "In file included from \u001b[01m\u001b[K/home/clive/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1822\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/home/clive/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/home/clive/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Kpopcount.c:610\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/home/clive/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
      "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
      "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n"
     ]
    }
   ],
   "source": [
    "# Note: -O3 -march=native isn't just for fun, you really do need it to beat NumPy.\n",
    "! which cython || conda install -y cython\n",
    "! test ! -d $$CONDA_PREFIX/envs/myenv/lib/python3.6/site-packages/blosc || conda install -y python-blosc\n",
    "! cython popcount.pyx\n",
    "! sed -i '/0xbad0bad0/d' popcount.c\n",
    "CXX=\"g++\"\n",
    "! $CXX --version\n",
    "#! g++ -Wall -O3 -Ofast -g -flto -lm -L$$CONDA_PREFIX/lib -lblosc -shared -pthread -fPIC -funroll-loops -fno-strict-aliasing -march=native -mno-avx256-split-unaligned-load -fopt-info-vec-optimized -I$$CONDA_PREFIX/include/python3.6m -I$$CONDA_PREFIX/include -I$$CONDA_PREFIX/lib/python3.6/site-packages/numpy/core/include/ -o popcount.so popcount.c\n",
    "! $CXX -Wall -Ofast -g -flto -fPIC -funroll-loops -fopenmp -fwrapv -fno-strict-aliasing -march=native -I$$CONDA_PREFIX/include/python3.7m -I$$CONDA_PREFIX/include -I$$CONDA_PREFIX/lib/python3.7/site-packages/numpy/core/include -c -o popcount.o popcount.c\n",
    "! $CXX -Wall -Ofast -g -flto -shared -pthread -fPIC -funroll-loops -fopenmp -fwrapv -fno-strict-aliasing -march=native -o popcount.so popcount.o -L$$CONDA_PREFIX/lib -Wl,-rpath=$$CONDA_PREFIX/lib -lblosc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy packed bits popcounted\" took 435.0261999934446 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import inplace_popcount_32\n",
    "with Timer(\"numpy packed bits popcounted\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    isct = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1) & np.packbits(query_f), dtype=np.uint32)\n",
    "    inplace_popcount_32(isct)\n",
    "    scores = np.sum(isct.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're processing more than 5x faster than it takes to read the database off the SSD, we can declare victory, but also I'm curious how far we can go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing operations\n",
    "This is sort of a diminishing returns scenario; I needed `-O3 -march=native` to run the bitwise and faster than NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy fused bitwise_and + popcount\" took 394.0944000205491 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and\n",
    "with Timer(\"numpy fused bitwise_and + popcount\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "    fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing the packing too\n",
    "The packbits function is now the slowest part, at about 80% of the runtime. What if we don't do that at all?\n",
    "\n",
    "It ends up being slower than NumPy because NumPy is [using proper SIMD instructions for this](https://github.com/numpy/numpy/blob/97d2db483fc0ffd46f38d0e1c39d5fc001e33197/numpy/core/src/multiarray/compiled_base.c#L1543). This is okay, we now have the tools to do this properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"fused notpacked popcount\" took 1220.9746999724302 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and_notpacked_count\n",
    "with Timer(\"fused notpacked popcount\", 1000000):\n",
    "    query_f = compute_fingerprint(query).astype(np.uint8)\n",
    "    fused_popcount_bitwise_and_notpacked_count(query_f, fingerprints[:1000000])\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endgame: AVX2\n",
    "AVX2 is supported by most modern processors, including the Intel processor that the specified MBP 2019 has.\n",
    "\n",
    "We get an absurdly high throughput: we're processing about 5 boolean database entries per nanosecond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"avx2\" took 123.41299999388865 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_avx2\n",
    "with Timer(\"avx2\", 1000000):\n",
    "    query_f = compute_fingerprint(query).astype(np.uint8)\n",
    "    scores = fused_popcount_avx2(query_f, fingerprints[:1000000]) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments and things to improve\n",
    "\n",
    "Algorithmic complexity is O(database size), since it must iterate over all the booleans in every row. I'm fairly certain that this cannot be asymptotically improved - for example in the worst case every molecule in the database contains all substructures (all 2048 bits are true).\n",
    "\n",
    "Memory usage is high; we're reading the entire database into memory and keeping it there. There isn't really any way around keeping O(database size) in memory because SSD bandwidth is far too low to support streaming from disk, but an 8x memory usage/bandwidth bottleneck improvement can probably be found by bitpacking the database beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Overtime stuff\n",
    "\n",
    "This was done for fun significantly over the 6hr time limit; please don't count this if you're evaluating quantity of code produced.\n",
    "\n",
    "## Pre-packing\n",
    "\n",
    "I did no memory-related optimizations mostly because I was more interested in playing with AVX, and because up until AVX the whole thing was compute-bottlenecked rather than memory-bottlenecked. However it's trivial to do so; just don't count the bitpacking step from the `Fusing operations` section, and you get (only!) a slight speedup over the AVX implementation as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 238.83260000729933 nanoseconds per db row\n",
      "\"pre-packed rows\" took 123.92220000037923 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "with Timer(\"pre-packed rows\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "    fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVX2 popcount attempt 1\n",
    "\n",
    "Not working correctly, but AVX2 emulated popcnt [can be 30% faster](https://stackoverflow.com/a/50082218) than the dedicated instruction.\n",
    "\n",
    "Edit: I think it won't be faster; according to [this paper](https://arxiv.org/pdf/1611.07612.pdf) and other sources, popcnt on 64-bit ints is the fastest way to do it at our data size of 2048 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 274.39370000502095 nanoseconds per db row\n",
      "\"pre-packed rows\" took 57.00010000145994 nanoseconds per db row\n",
      "[4.03833063e+15 2.62815452e+16 3.77945367e+15 3.34960946e+16\n",
      " 8.88579474e+15]\n",
      "[439306  28215  93292 489702 106223] [3.83505627e+16 3.83506231e+16 3.83505872e+16 3.83507498e+16\n",
      " 3.83504301e+16]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_avx2_emulated_popcount\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint64)\n",
    "    fused_avx2_emulated_popcount(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcount64 and some tweaks to get good assembly\n",
    "\n",
    "Let's use popcount64 directly on 64-bit ints, instead of 32-bit ints, and also do counting in the kernel itself. (Also added loop unrolling flag, for the slight benefit of all these examples.)\n",
    "\n",
    "Maybe let's be less guessy. Here's a relevant assembly sample from a single iteration of the unrolled loop.\n",
    "```\n",
    "   27c1a:       44 09 c2                or     %r8d,%edx\n",
    "   27c1d:       41 89 07                mov    %eax,(%r15)\n",
    "   27c20:       48 63 ca                movslq %edx,%rcx\n",
    "   27c23:       49 0f af cc             imul   %r12,%rcx\n",
    "   27c27:       49 8b 14 0b             mov    (%r11,%rcx,1),%rdx\n",
    "   27c2b:       48 23 17                and    (%rdi),%rdx\n",
    "   27c2e:       31 c9                   xor    %ecx,%ecx\n",
    "   27c30:       4c 01 cf                add    %r9,%rdi\n",
    "   27c33:       f3 48 0f b8 ca          popcnt %rdx,%rcx\n",
    "   27c38:       8d 56 02                lea    0x2(%rsi),%edx\n",
    "   27c3b:       01 c8                   add    %ecx,%eax\n",
    "```\n",
    "There might be a tiny bit of gain to be had by avoiding some of these instructions. I don't fully understand what they do, though.\n",
    "\n",
    "Making `count` as a separate int variable (instead of accumulating directly into `counts[i>>5]`) results in much less consistent loop unrolling - there doesn't actually seem to be a particular pattern to the iterations; they have a bunch of random instructions squeezed in between that don't necessarily repeat. Finally, making `fingerprints_packed_curr` as a temporary variable instead of calculating `i|j` every time makes the loop much tighter:\n",
    "```\n",
    "   27d17:       4c 8b 79 d0             mov    -0x30(%rcx),%r15    ; Get a quadword of fingerprints_packed_curr (fixed offset)\n",
    "   27d1b:       4c 23 3a                and    (%rdx),%r15         ; AND it with a query quadword\n",
    "   27d1e:       48 01 f2                add    %rsi,%rdx           ; Advance the query pointer\n",
    "   27d21:       f3 4d 0f b8 ff          popcnt %r15,%r15           ; Popcount it\n",
    "   27d26:       41 01 c7                add    %eax,%r15d          ; Add this to the count\n",
    "```\n",
    "Those two modifications combined give an additional 25-30% better performance. I don't think there's anything else that can be done to speed this up, unless AVX2 has some real magic to offer (but even then, the frequency hit from AVX/AVX2 probably means that it won't be able to overcome this).\n",
    "\n",
    "The fastest I've seen for just the `fused_popcount64_bitwise_and` part is 30ns:\n",
    "- 2048bits/30ns = 8.5 GBps, out of a memory bandwidth of ~10-20GBps.\n",
    "- 32 popcounts / 30ns = approx one 64-bit popcount per 0.9 nanoseconds. This almost double the expected CPI of 5 cycles @ 3GHz = 1.6 ns, assuming the 5 listed instructions above take 1 cycle each, but apparently this is very superscalar, or my math is wrong (is frequency == cycles per second?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 212.23840001039207 nanoseconds per db row\n",
      "\"pre-packed rows\" took 27.69667999818921 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='little'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows\", 10000000):\n",
    "    for i in range(10):\n",
    "        query_f = compute_fingerprint(query)\n",
    "        query_packed = np.frombuffer(np.packbits(query_f, bitorder='little'), dtype=np.uint64)\n",
    "        counts = fused_popcount64_bitwise_and(query_packed, fingerprints_packed)\n",
    "        total = fused_popcount64_bitwise_and(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "        topk = np.argpartition(-counts, k)[:k]\n",
    "print(counts[:k]/total)\n",
    "print(topk, counts[topk]/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVX2 attempt 2: success and maximum throughput\n",
    "\n",
    "It might be possible to use AVX2 instructions to MOV or AND or ADD more efficiently; 10ns is the absolute limit because the popcount instruction has a CPI of 1. I'm highly doubtful it'll make things faster given that the width is only 256 bits anyway though, and there has to be some penalty moving between AVX registers and the regular registers where popcnt lives, though maybe that can be pipelined away.\n",
    "- LOADU twice for 256 bits each\n",
    "- AND 256 bits together\n",
    "- _mm256_extract_epi64 and popcount each of the 4 registers, adding\n",
    "\n",
    "With some guidance from [this](https://github.com/WojciechMula/sse-popcount/blob/master/popcnt-avx2-lookup.cpp) we can get a 20% speed boost.\n",
    "\n",
    "It turns out that even though benchmarks show unrolled loop of int64 popcnt is the fastest popcount implementation for 256 bytes, for our usecase we have additional math alongside it (the AND) so putting everything into AVX is actually beneficial.\n",
    "\n",
    "Our inner loop looks something like (\"something like\" because loop unrolling has made things confusing) this:\n",
    "```\n",
    "   28b23:       c4 43 25 38 6d 10 01    vinserti128 $0x1,0x10(%r13),%ymm11,%ymm13\n",
    "   28b2a:       c5 d5 71 d4 04          vpsrlw $0x4,%ymm4,%ymm5\n",
    "   28b2f:       c4 63 35 38 54 07 50    vinserti128 $0x1,0x50(%rdi,%rax,1),%ymm9,%ymm10\n",
    "   28b36:       01 \n",
    "   28b37:       c4 c1 7d fc c7          vpaddb %ymm15,%ymm0,%ymm0\n",
    "   28b3c:       c5 6d db c5             vpand  %ymm5,%ymm2,%ymm8\n",
    "   28b40:       c5 dd db f2             vpand  %ymm2,%ymm4,%ymm6\n",
    "   28b44:       c4 c1 15 db ca          vpand  %ymm10,%ymm13,%ymm1\n",
    "   28b49:       c4 42 1d 00 f0          vpshufb %ymm8,%ymm12,%ymm14\n",
    "   28b4e:       c4 41 7a 6f 14 24       vmovdqu (%r12),%xmm10\n",
    "   28b54:       c4 e2 1d 00 fe          vpshufb %ymm6,%ymm12,%ymm7\n",
    "   28b59:       c5 7a 6f 44 07 60       vmovdqu 0x60(%rdi,%rax,1),%xmm8\n",
    "```\n",
    "Even though it's longer, it's also handling 4x as much data per iteration.\n",
    "\n",
    "Turning on `-march=native -mno-avx256-split-unaligned-load` gets us to something like\n",
    "```\n",
    "   289ac:       c4 41 45 db cd          vpand  %ymm13,%ymm7,%ymm9\n",
    "   289b1:       c5 7d fc fe             vpaddb %ymm6,%ymm0,%ymm15\n",
    "   289b5:       c5 ed db bc 07 a0 00    vpand  0xa0(%rdi,%rax,1),%ymm2,%ymm7\n",
    "   289bc:       00 00 \n",
    "   289be:       c4 c1 05 fc f6          vpaddb %ymm14,%ymm15,%ymm6\n",
    "   289c3:       c4 e2 1d 00 e1          vpshufb %ymm1,%ymm12,%ymm4\n",
    "   289c8:       c4 c1 15 db c8          vpand  %ymm8,%ymm13,%ymm1\n",
    "   289cd:       c4 c2 1d 00 d9          vpshufb %ymm9,%ymm12,%ymm3\n",
    "   289d2:       c5 bd 71 d7 04          vpsrlw $0x4,%ymm7,%ymm8\n",
    "```\n",
    "Where we only see instructions that we've specified explicitly in the code. In fact, we're missing a bunch of `vmovdqu`'s - the compiler is doing something clever here that I don't understand.\n",
    "\n",
    "- 2048bits/22ns = 11.6 GBps, out of a memory bandwidth of ~10-20GBps.\n",
    "- ~80 AVX instructions / 22ns = approx 3.6GHz, which is significantly higher than the clock frequency. This is probably because some instructions can have a throughput of more than 1 per cycle.\n",
    "\n",
    "This may well be the end of the line for optimizing this, as this does seem to be optimal (all sources point to Mula as the source of these algorithms, and this is the best his papers have to offer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 207.71160000003874 nanoseconds per db row\n",
      "\"pre-packed rows 1\" took 0.9360399999422953 nanoseconds per db row\n",
      "\"pre-packed rows 2\" took 0.023940001847222447 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.461230001412332 nanoseconds per db row\n",
      "\"pre-packed rows 4\" took 0.004290000651963055 nanoseconds per db row\n",
      "\"pre-packed rows 5\" took 4.68531999795232 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='little'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 1\", 10000000):\n",
    "    for i in range(10):\n",
    "        query_f = compute_fingerprint(query)\n",
    "with Timer(\"pre-packed rows 2\", 10000000):\n",
    "    for i in range(10):\n",
    "        query_packed = np.frombuffer(np.packbits(query_f, bitorder='little'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 3\", 10000000):\n",
    "    for i in range(10):\n",
    "        counts = fused_popcount64_bitwise_and_avx(query_packed, fingerprints_packed)\n",
    "with Timer(\"pre-packed rows 4\", 10000000):\n",
    "    for i in range(10):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "with Timer(\"pre-packed rows 5\", 10000000):\n",
    "    for i in range(10):\n",
    "        topk = np.argpartition(-counts, k)[:k]\n",
    "print(counts[:k]/total)\n",
    "print(topk, counts[topk]/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finale: Optimizing other parts of the system\n",
    "\n",
    "We're actually getting to the point where the final np.argpartition on 1 million elements is taking about 25% of the total time. Using np.argsort is 10x slower. We did actually need this initially seemingly unnecessary optimization!\n",
    "\n",
    "There are [ways](https://github.com/WojciechMula/simd-sort) to make this happen faster, but since we're going to be generating the entire count array anyway, why not just construct it on the fly? Specifically, let's use a **min-heap of size k** to track the k largest elements we've seen.\n",
    "\n",
    "This absorbs the topk algorithm without any measurable performance penalty (less than 10% with k=1000). A bit of tweaking suffices to double the speed of the compute_fingerprint, and now we are truly at the end of the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 248.92470001941547 nanoseconds per db row\n",
      "\"pre-packed rows 0\" took 0.1470180001342669 nanoseconds per db row\n",
      "\"pre-packed rows 1\" took 0.32450899976538494 nanoseconds per db row\n",
      "\"pre-packed rows 2\" took 0.015281999949365854 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.00739999068901 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.381500016199425 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.747099987696856 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 19.762799987802282 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.822899990482256 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.291199997998774 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.290799994952977 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.846799990162253 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.25640000565909 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.6742000146769 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.57030000933446 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.196300002979115 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.350700014503673 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.85780002642423 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.669200005708262 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.89490000717342 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.44150000740774 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.07549998536706 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.21340000606142 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 19.018699997104704 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.636300006415695 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.45490002213046 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.20739998947829 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 20.16000001458451 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 20.249799999874085 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 20.262999983970076 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 22.39269998972304 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 20.807099994271994 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 20.060600014403462 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 19.999700016342103 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.596799985971302 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.673399987164885 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.553000001702458 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.69350000540726 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.522300011478364 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 19.876400008797646 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.45499999378808 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.332099989289418 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.922400012845173 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.734199995175004 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.852100001415238 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.566900001838803 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.991799988318235 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.013600008795038 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.285100009758025 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.862500022398308 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.965499991783872 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.56269998056814 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.940200002864003 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.133699991973117 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.609299993840978 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.05650001531467 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.550099990330637 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.018799990182742 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.30639999732375 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.654199997195974 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.06079998984933 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.231600009836257 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.59539999673143 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.470799997681752 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.4057999975048 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.87120001972653 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.06529999501072 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.30240001436323 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.261500004678965 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.35989999724552 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.124100006185472 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.156599991722032 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.991699987556785 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.153699988033623 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.912899988004938 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.122500012395903 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.357299977447838 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.58659997722134 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.415499984053895 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.22519999020733 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.280300002312288 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.05320001929067 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.514299979666248 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.59729998977855 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 19.42850000341423 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 21.180500014452264 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 21.895299985772 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.858300008811057 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.85200001904741 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.44450000114739 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.13170000584796 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.34680000320077 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.328499990981072 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.08449999568984 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.15679998253472 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.837299994425848 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.495099978987128 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.53899999312125 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.175400018459186 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 19.67139999032952 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.13949999632314 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.889300012029707 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 21.787600009702146 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 20.19260000088252 nanoseconds per db row\n",
      "\"pre-packed rows 4\" took 0.0025489999097771943 nanoseconds per db row\n",
      "[324758 329123 324807 324852 329172] [0.66735967 0.66735967 0.66735967 0.66943867 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx_topk, fused_popcount64_bitwise_and_avx\n",
    "from rdkit import Chem\n",
    "\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='big'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 0\", 100000000):\n",
    "    for i in range(100):\n",
    "        mol = Chem.MolFromSmiles(query)\n",
    "with Timer(\"pre-packed rows 1\", 100000000):\n",
    "    for i in range(100):\n",
    "        s = Chem.RDKFingerprint(mol, fpSize=2048, maxPath=5).ToBitString()\n",
    "with Timer(\"pre-packed rows 2\", 100000000):\n",
    "    for i in range(100):\n",
    "        query_packed = np.copy(np.frombuffer(int(s, 2).to_bytes(len(s) // 8, byteorder='big'), dtype=np.uint64))\n",
    "for i in range(100):\n",
    "    with Timer(\"pre-packed rows 3\", 1000000):\n",
    "        counts, topk = fused_popcount64_bitwise_and_avx_topk(query_packed, fingerprints_packed, k)\n",
    "with Timer(\"pre-packed rows 4\", 100000000):\n",
    "    for i in range(100):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "print(topk, counts/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenMP Threading\n",
    "\n",
    "After some OpenMP we can smash the 20ns barrier.\n",
    "\n",
    "\"personal best\" = \"5.8700999999814485 nanoseconds per db row\" for `fused_popcount64_bitwise_and_avx_topk_omp`\n",
    "\n",
    "3600MHz RAM, 2048 bits / 5.869199999978036 ns = 43.6175288 GBps, after moving to my Ryzen 3700x machine which should have a max of 28.8GBps per stick, times two sticks.\n",
    "\n",
    "Going from 2133MHz to 3600MHz speeds up by 50%, and overclocking CPU doesn't visibly help, so it's legitimately memory bandwidth bound, and even after the frequency increase to 3600MHz it's still probably memory bound. That's pretty wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 247.27249998250045 nanoseconds per db row\n",
      "\"pre-packed rows 0\" took 0.15130400017369539 nanoseconds per db row\n",
      "\"pre-packed rows 1\" took 0.3249610000057146 nanoseconds per db row\n",
      "\"pre-packed rows 2\" took 0.014942999987397343 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 12.409160000970587 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.690320000052452 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.4921800018055364 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.844000000273809 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.584119998500682 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.036680000601336 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.063479998032562 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.535829999484122 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.469980001566 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.811080002808012 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.6672899993136525 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.802679999964312 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.038230000762269 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.484220000333152 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.4088399996981025 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.303309999406338 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.150620000902563 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.875830002012662 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 8.233830000972375 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 8.347250000224449 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 8.095880001201294 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 8.642399997916073 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 10.4258800012758 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 8.414659998379648 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.906599999638274 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.541199999628589 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.525840001995675 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.0812299993122 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.8261300009908155 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.704090000130236 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.203460001619533 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 12.965669998084195 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.8518499998608595 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.632580000790768 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.5655799999367455 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.257730001583695 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.212699999217875 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.206410000799224 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.3503199993865564 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.865010000183247 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 10.768929999903776 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.232979999389499 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.922520001535304 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.835940000019036 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.216550002340227 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.86989999958314 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.9002100004581735 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 8.827880001626909 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 10.023129999171942 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 11.995690001640469 nanoseconds per db row\n",
      "\"pre-packed rows 4\" took 0.0030819998937658966 nanoseconds per db row\n",
      "[324758 324807 329123 324852 329172] [0.66735967 0.66735967 0.66735967 0.66943867 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx_topk_omp, fused_popcount64_bitwise_and_avx\n",
    "from rdkit import Chem\n",
    "\n",
    "import os\n",
    "# This only works before the first run of openmp in a process\n",
    "os.environ[\"OMP_PLACES\"] = \"cores\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='big'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 0\", 100000000):\n",
    "    for i in range(100):\n",
    "        mol = Chem.MolFromSmiles(query)\n",
    "with Timer(\"pre-packed rows 1\", 100000000):\n",
    "    for i in range(100):\n",
    "        s = Chem.RDKFingerprint(mol, fpSize=2048, maxPath=5).ToBitString()\n",
    "with Timer(\"pre-packed rows 2\", 100000000):\n",
    "    for i in range(100):\n",
    "        query_packed = np.copy(np.frombuffer(int(s, 2).to_bytes(len(s) // 8, byteorder='big'), dtype=np.uint64))\n",
    "for i in range(50):\n",
    "    with Timer(\"pre-packed rows 3\", 10000000):\n",
    "        for i in range(10):\n",
    "            counts, topk = fused_popcount64_bitwise_and_avx_topk_omp(query_packed, fingerprints_packed, k)\n",
    "with Timer(\"pre-packed rows 4\", 100000000):\n",
    "    for i in range(100):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "print(topk, counts/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed: blosc\n",
    "Since we might have a memory bottleneck, does blosc help?\n",
    "\n",
    "Nope, that was a total failure. Maybe next step is to try out custom encodings like RLE or even sparse bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression: 0.70317598828125\n"
     ]
    }
   ],
   "source": [
    "import blosc\n",
    "fingerprints_blosc_compressed = np.frombuffer(blosc.compress(fingerprints_packed, typesize=8, shuffle=blosc.NOSHUFFLE, cname='blosclz'), dtype=np.uint8)\n",
    "\n",
    "print(\"Compression:\", len(fingerprints_blosc_compressed)/8/len(fingerprints_packed))\n",
    "# %timeit blosc.decompress(fingerprints_blosc_compressed)\n",
    "# %timeit np.copy(fingerprints_packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 605.843100027414 nanoseconds per db row\n",
      "\"pre-packed rows 0\" took 0.2631999959703535 nanoseconds per db row\n",
      "\"pre-packed rows 1\" took 0.3476999991107732 nanoseconds per db row\n",
      "\"pre-packed rows 2\" took 0.05319999763742089 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 61.555499996757135 nanoseconds per db row\n",
      "\"pre-packed rows 4\" took 0.020899984519928694 nanoseconds per db row\n",
      "[ 999437 1003532  999438 1003533 1003534] [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx_topk_omp_blosc, fused_popcount64_bitwise_and_avx\n",
    "from rdkit import Chem\n",
    "import blosc\n",
    "\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints, axis=1, bitorder='big'), dtype=np.uint64)\n",
    "    fingerprints_blosc_compressed = np.frombuffer(blosc.compress(fingerprints_packed, typesize=8, shuffle=blosc.NOSHUFFLE, cname='blosclz'), dtype=np.uint8)\n",
    "    n_fingerprints = len(fingerprints)\n",
    "with Timer(\"pre-packed rows 0\", 1000000):\n",
    "    for i in range(1):\n",
    "        mol = Chem.MolFromSmiles(query)\n",
    "with Timer(\"pre-packed rows 1\", 1000000):\n",
    "    for i in range(1):\n",
    "        s = Chem.RDKFingerprint(mol, fpSize=2048, maxPath=5).ToBitString()\n",
    "with Timer(\"pre-packed rows 2\", 1000000):\n",
    "    for i in range(1):\n",
    "        query_packed = np.copy(np.frombuffer(int(s, 2).to_bytes(len(s) // 8, byteorder='big'), dtype=np.uint64))\n",
    "with Timer(\"pre-packed rows 3\", 1000000):\n",
    "    for i in range(1):\n",
    "        counts, topk = fused_popcount64_bitwise_and_avx_topk_omp_blosc(query_packed, fingerprints_blosc_compressed, k, n_fingerprints)\n",
    "with Timer(\"pre-packed rows 4\", 1000000):\n",
    "    for i in range(1):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "print(topk, counts/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The real endgame: CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycuda in /home/clive/.local/lib/python3.7/site-packages (2020.1)\r\n",
      "Requirement already satisfied: pytools>=2011.2 in /home/clive/.local/lib/python3.7/site-packages (from pycuda) (2020.4)\r\n",
      "Requirement already satisfied: decorator>=3.2.0 in /home/clive/.local/lib/python3.7/site-packages (from pycuda) (4.4.2)\r\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /home/clive/.local/lib/python3.7/site-packages (from pycuda) (1.4.4)\r\n",
      "Requirement already satisfied: mako in /home/clive/.local/lib/python3.7/site-packages (from pycuda) (1.1.3)\r\n",
      "Requirement already satisfied: six>=1.8.0 in /home/clive/.local/lib/python3.7/site-packages (from pytools>=2011.2->pycuda) (1.15.0)\r\n",
      "Requirement already satisfied: numpy>=1.6.0 in /home/clive/.local/lib/python3.7/site-packages (from pytools>=2011.2->pycuda) (1.19.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/clive/.local/lib/python3.7/site-packages (from mako->pycuda) (1.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda.compiler import SourceModule\n",
    "from pycuda.gpuarray import GPUArray, to_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000000,)\n",
      "\"pre-packing process\" took 318.59070001519285 nanoseconds per db row\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed_gpu = to_gpu(np.frombuffer(np.packbits(fingerprints, axis=1, bitorder='big'), dtype=np.uint64))\n",
    "    print(fingerprints_packed_gpu.shape)\n",
    "    sums_gpu = GPUArray((100,), dtype=np.int32)\n",
    "    n_fingerprints = len(fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void and_popcount(unsigned long long *a, unsigned long long *b, int *sums)\n",
    "  {\n",
    "    int START = (blockIdx.x * blockDim.x + threadIdx.x) * 1000;\n",
    "    int END = min((blockIdx.x * blockDim.x + threadIdx.x + 1) * 1000, 1000000);\n",
    "    int sum = 0;\n",
    "    for (int i = START; i < START+1; i++) {\n",
    "      unsigned long long *a_tmp = &a[i<<5];\n",
    "      for (int j = 0; j < 32; j++) {\n",
    "        sum += __popcll(a_tmp[j] & b[j]);\n",
    "      }\n",
    "    }\n",
    "    sums[blockDim.x * blockIdx.x + threadIdx.x] = sum;\n",
    "  }\n",
    "  \"\"\", options=['--use_fast_math', '-O3', '-Xptxas', '-O3,-v'])\n",
    "and_popcount = mod.get_function(\"and_popcount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n"
     ]
    },
    {
     "ename": "LogicError",
     "evalue": "cuModuleLoadDataEx failed: an illegal memory access was encountered - ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pycuda/tools.py\u001b[0m in \u001b[0;36mcontext_dependent_memoize\u001b[0;34m(func, *args)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mctx_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_ctx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: <pycuda._driver.Context object at 0x7fb8a81a4990>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLogicError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f76659c9dd27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mand_popcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfingerprints_packed_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_packed_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msums_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpycuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpuarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msums_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pycuda/gpuarray.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, dtype, stream, allocator)\u001b[0m\n\u001b[1;32m   1638\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallocator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpycuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_sum_kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1640\u001b[0;31m     \u001b[0mkrnl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sum_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1641\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mkrnl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallocator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallocator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-151>\u001b[0m in \u001b[0;36mget_sum_kernel\u001b[0;34m(dtype_out, dtype_in)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pycuda/tools.py\u001b[0m in \u001b[0;36mcontext_dependent_memoize\u001b[0;34m(func, *args)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mcontext_dependent_memoized_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0marg_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m         \u001b[0marg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pycuda/reduction.py\u001b[0m in \u001b[0;36mget_sum_kernel\u001b[0;34m(dtype_out, dtype_in)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     return ReductionKernel(dtype_out, \"0\", \"a+b\",\n\u001b[0;32m--> 305\u001b[0;31m             arguments=\"const %(tp)s *in\" % {\"tp\": dtype_to_ctype(dtype_in)})\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pycuda/reduction.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dtype_out, neutral, reduce_expr, map_expr, arguments, name, keep, options, preamble)\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mneutral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_expr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_stage1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 preamble=preamble)\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage1_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepared_async_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pycuda/reduction.py\u001b[0m in \u001b[0;36mget_reduction_kernel_and_types\u001b[0;34m(stage, out_type, block_size, neutral, reduce_expr, map_expr, arguments, name, keep, options, preamble)\u001b[0m\n\u001b[1;32m    176\u001b[0m     mod = get_reduction_module(out_type, block_size,\n\u001b[1;32m    177\u001b[0m             \u001b[0mneutral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             name, keep, options, preamble)\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpycuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_arg_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pycuda/reduction.py\u001b[0m in \u001b[0;36mget_reduction_module\u001b[0;34m(out_type, block_size, neutral, reduce_expr, map_expr, arguments, name, keep, options, preamble)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;34m\"preamble\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreamble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             }\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mSourceModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_extern_c\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pycuda/compiler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpycuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_from_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_from_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcubin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLogicError\u001b[0m: cuModuleLoadDataEx failed: an illegal memory access was encountered - "
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "mol = Chem.MolFromSmiles(query)\n",
    "s = Chem.RDKFingerprint(mol, fpSize=2048, maxPath=5).ToBitString()\n",
    "query_packed_gpu = to_gpu(np.frombuffer(int(s, 2).to_bytes(len(s) // 8, byteorder='big'), dtype=np.uint64))\n",
    "print(query_packed_gpu.shape)\n",
    "\n",
    "and_popcount(fingerprints_packed_gpu, query_packed_gpu, sums_gpu, grid=(32,1,1), block=(32,1,1))\n",
    "pycuda.gpuarray.sum(sums_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "with Timer(\"pre-packed rows 0\", 1000000):\n",
    "    for i in range(1):\n",
    "        mol = Chem.MolFromSmiles(query)\n",
    "with Timer(\"pre-packed rows 1\", 1000000):\n",
    "    for i in range(1):\n",
    "        s = Chem.RDKFingerprint(mol, fpSize=2048, maxPath=5).ToBitString()\n",
    "with Timer(\"pre-packed rows 2\", 1000000):\n",
    "    for i in range(1):\n",
    "        query_packed = np.copy(np.frombuffer(int(s, 2).to_bytes(len(s) // 8, byteorder='big'), dtype=np.uint64))\n",
    "with Timer(\"pre-packed rows 3\", 1000000):\n",
    "    for i in range(1):\n",
    "        counts, topk = fused_popcount64_bitwise_and_avx_topk_omp_blosc(query_packed, fingerprints_blosc_compressed, k, n_fingerprints)\n",
    "with Timer(\"pre-packed rows 4\", 1000000):\n",
    "    for i in range(1):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "print(topk, counts/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future stuff\n",
    "AVX-512 would be fun but I don't have ice lake hardware sadly. This should be something like 3x faster, less any AVX-induced processor frequency hit.\n",
    "- `_mm512_loadu_epi64`\n",
    "- `_mm512_and_epi64`\n",
    "- `_mm512_popcnt_epi64`\n",
    "- `_mm512_reduce_add_epi64`\n",
    "\n",
    "Some additional cleverness that I do not have time to explore, but which is unlikely to yield better results:\n",
    "- Database rows are almost always sparse. There might be more efficient popcount algorithms in this case.\n",
    "- OpenCL was originally planned but no time, and also I think my laptop iGPU doesn't support it anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Threading\n",
    "Because why not? The AVX2 code is mostly in C-land away from the GIL so we might be able to get close to linear speedup in the number of cores (regular cores; hyperthreading probably does not help for this because AVX2 resources are shared).\n",
    "\n",
    "This didn't help at all with the AVX2 code because there are other bottlenecks e.g. system memory bandwidth. 2GB dataset in 0.2s is 10GBps, which is around half the theoretical memory bandwidth my machine could have, but there's probably some other caveats there.\n",
    "\n",
    "This does help a lot (+50% ish) with regular popcount64 because of the 8x lower memory bandwidth requirement when bits are packed. 2GB/8 = 256MB dataset in 0.08s is only 3GBps, so it's probably still compute limited and could benefit from more threading. Unfortunately I only have 2 actual cores (4 hyperthreaded) so there can be no more than a 2x speedup.\n",
    "\n",
    "However modifying popcount64 to fuse the counting step as well results in having no significant threading benefit. Occasionally it will run faster than single-threaded, but most of the time it's running into some issue or another - maybe memory bandwidth, maybe scheduler issues because it's so fast.\n",
    "\n",
    "This also points toward there being very little point in using OpenCL: unless you have a discrete GPU with dedicated high bandwidth video memory, which isn't the case on MBP 2019, it's not going to get you much further, and more likely will slow things down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from popcount import fused_popcount64_bitwise_and\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='little'), dtype=np.uint64)\n",
    "\n",
    "core_ids = set()\n",
    "with open('/proc/cpuinfo') as f:\n",
    "    for line in f:\n",
    "        if line.count('core id'):\n",
    "            core_ids.add(line)\n",
    "\n",
    "nthreads = len(core_ids)\n",
    "assert(len(fingerprints) % nthreads == 0) # Note: below code calculating start and end assumes that nthreads divides len(fingerprints)\n",
    "print(f\"Using {nthreads} threads\")\n",
    "\n",
    "topk_per_thread = [None] * nthreads\n",
    "topk_per_thread_scores = [None] * nthreads\n",
    "\n",
    "def thread_func(query_packed, threadid):\n",
    "    start = threadid*len(fingerprints_packed)//nthreads\n",
    "    end = (threadid+1)*len(fingerprints_packed)//nthreads\n",
    "    \n",
    "    counts = fused_popcount64_bitwise_and(query_packed, fingerprints_packed[start:end])\n",
    "    total = fused_popcount64_bitwise_and(query_packed, query_packed)\n",
    "\n",
    "    topk_per_thread[threadid] = np.argpartition(-counts, k)[:k]\n",
    "\n",
    "    topk_per_thread_scores[threadid] = counts[topk_per_thread[threadid]] / total\n",
    "    topk_per_thread[threadid] += start // 32\n",
    "\n",
    "with Timer(\"threaded popcount64\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f, bitorder='little'), dtype=np.uint64)\n",
    "    threads = [None] * nthreads\n",
    "    for i in range(1, nthreads):\n",
    "        threads[i] = threading.Thread(target=thread_func, args=(query_packed, i))\n",
    "        threads[i].start()\n",
    "    thread_func(query_packed, 0)\n",
    "    for i in range(1, nthreads):\n",
    "        threads[i].join()\n",
    "    topk_arg = np.argsort(-np.concatenate(topk_per_thread_scores))[:k]\n",
    "    scores = np.concatenate(topk_per_thread_scores)[topk_arg]\n",
    "    topk = np.concatenate(topk_per_thread)[topk_arg]\n",
    "print(topk, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: A failed attempt at using Numba\n",
    "Cython has some ... interoperability issues ... with Numba, so using the popcount kernel isn't possible.\n",
    "\n",
    "Using purely Numba (without the popcount kernel) is way slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from numba import jit\n",
    "except ImportError:\n",
    "    ! conda install -y numba\n",
    "    from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "# from numba.extending import get_cython_function_address\n",
    "# import numpy.ctypeslib as npct\n",
    "# import ctypes\n",
    "\n",
    "# array_1d_uint32 = npct.ndpointer(dtype=np.uint32, ndim=1, flags='CONTIGUOUS')\n",
    "# addr = get_cython_function_address(\"popcount\", \"_fused_popcount_bitwise_and\")\n",
    "# functype = ctypes.CFUNCTYPE(None, array_1d_uint32, array_1d_uint32)\n",
    "# fused_popcount_bitwise_and = functype(addr)\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def numba_func(query_f, fingerprints):\n",
    "    return np.sum(query_f & fingerprints[:100000], axis=1) / np.sum(query_f)\n",
    "\n",
    "# @numba.jit(nopython=True)\n",
    "# def numba_func(query_f, fingerprints):\n",
    "#     query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "#     fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "#     fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "#     return np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "\n",
    "with Timer(\"numba\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = numba_func(query_f, fingerprints)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import popcount\n",
    "popcount = importlib.reload(popcount)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
