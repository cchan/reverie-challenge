{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class Timer:\n",
    "    def __init__(self, s, iters):\n",
    "        self.s = s\n",
    "        self.iters = iters\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        print(\"\\\"\" + self.s + \"\\\"\", \"took\", (time.perf_counter() - self.start)/self.iters*1e9, \"nanoseconds per db row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"db loading\" took 3400.641900007031 nanoseconds per db row\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from starter import compute_fingerprint, load_database\n",
    "\n",
    "with Timer(\"db loading\", 1000000):\n",
    "    fingerprints = load_database(\"database_fingerprints.npy\")\n",
    "    molecules = pd.read_csv(\"database.csv\")\n",
    "with open(\"query.txt\") as q:\n",
    "  query = q.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup\n",
    "Just python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"python coverage function\" took 2303890.7000009203 nanoseconds per db row\n",
      "[0.2515592515592516, 0.22245322245322247, 0.2785862785862786, 0.22453222453222454, 0.30353430353430355]\n",
      "[928  66  59 429 428] [0.4261954261954262, 0.4303534303534304, 0.4365904365904366, 0.46361746361746364, 0.498960498960499]\n"
     ]
    }
   ],
   "source": [
    "def coverage(query, ref):\n",
    "  query_f = compute_fingerprint(query)\n",
    "  ref_f = compute_fingerprint(ref)\n",
    "  return sum(query_f & ref_f) / sum(query_f)\n",
    "\n",
    "with Timer(\"python coverage function\", 1000):\n",
    "    scores = [coverage(query, molecules[\"smiles\"][i]) for i in range(1000)]\n",
    "    topk = np.argsort(scores)[-k:]\n",
    "print(scores[:k])\n",
    "print(topk, [scores[i] for i in topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some sanity checks before we proceed\n",
    "After we check that these scores are meaningful, we can proceed and just verify that our scores are exactly identical to this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.746268656716418\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(coverage(molecules[\"smiles\"][0], molecules[\"smiles\"][0])) # should be 1\n",
    "print(coverage(molecules[\"smiles\"][0], molecules[\"smiles\"][1])) # should be between 0 and 1\n",
    "print(coverage(\"c1ccccc1\", \"C1CCCCC1\")) # should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed things up with NumPy\n",
    "We can get a massive speedup by using NumPy; everything is automatically vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy direct bitwise_and\" took 6687.390999941272 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[94115 39023 94179 94185 38955] [0.60914761 0.59459459 0.6029106  0.6008316  0.5966736 ]\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"numpy direct bitwise_and\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = np.sum(query_f & fingerprints[:100000], axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packing bits\n",
    "Packing bits allows us to `&` 8 bits at a time.\n",
    "\n",
    "Unpacking and summing is extremely inefficient though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy packed bits bitwise_and\" took 2020.8860000275308 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[94115 39023 94179 94185 38955] [0.60914761 0.59459459 0.6029106  0.6008316  0.5966736 ]\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"numpy packed bits bitwise_and\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = np.sum(np.unpackbits(np.packbits(fingerprints[:100000], axis=1) & np.packbits(query_f)).reshape(-1, 2048), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython\n",
    "We need C++ to get even faster; I want to use the native popcount instruction because [NumPy doesn't have a function to do that yet](https://github.com/numpy/numpy/issues/16325)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/clive/anaconda3/envs/myenv/bin/cython\n",
      "/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /home/clive/code/reverie-challenge/popcount.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "In file included from \u001b[01m\u001b[K/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/numpy/core/include/numpy/ndarraytypes.h:1822\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Kpopcount.c:610\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/home/clive/anaconda3/envs/myenv/lib/python3.6/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
      "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
      "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
      "popcount.c:27599:25: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:27195:26: optimized: basic block part vectorized using 32 byte vectors\n",
      "popcount.c:23142:24: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28009:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28009:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28055:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:5171:3: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28009:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28009:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28032:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28009:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28009:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28032:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28009:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28009:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28055:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:24090:9: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:5298:19: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:5715:3: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:27917:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:27917:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:27940:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:27195:26: optimized: basic block part vectorized using 32 byte vectors\n",
      "popcount.c:27917:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:27986:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:27195:26: optimized: basic block part vectorized using 32 byte vectors\n",
      "popcount.c:3497:101: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:27917:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:3503:3: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:3506:3: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:3543:3: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28009:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28009:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28078:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28055:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:5846:19: optimized: basic block part vectorized using 32 byte vectors\n",
      "popcount.c:6342:3: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28009:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:28009:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:27917:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:25355:47: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:24058:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:18011:33: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:18011:33: optimized:  loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:18011:33: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:18011:33: optimized:  loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:24058:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:24436:29: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:23949:16: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:23806:22: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:8657:39: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:7996:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:8054:19: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:8399:19: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:22435:19: optimized: basic block part vectorized using 32 byte vectors\n",
      "popcount.c:22450:19: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:20613:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:20652:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:18803:50: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:19110:33: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:18803:50: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:20149:33: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:20149:33: optimized:  loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:20149:33: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:20149:33: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:20149:33: optimized:  loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:20149:33: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:19272:15: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:19070:33: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:9722:36: optimized: basic block part vectorized using 32 byte vectors\n",
      "popcount.c:26211:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:26209:18: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:18011:33: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:18011:33: optimized:  loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:18011:33: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:18011:33: optimized:  loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:17693:69: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:17804:3: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:15407:50: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:23525:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:13982:15: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:12631:44: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:12631:44: optimized:  loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:12577:19: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:23806:22: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:24386:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:24386:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:12272:21: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:23806:22: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:23074:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:24386:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:24386:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:24068:30: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:24386:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:12066:37: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:24058:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:10146:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:10211:3: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:6109:5: optimized: basic block part vectorized using 32 byte vectors\n",
      "popcount.c:6157:17: optimized: basic block part vectorized using 32 byte vectors\n",
      "popcount.c:5533:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:5572:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:4989:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:5028:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:4558:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:4569:28: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:4588:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:4207:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:4218:28: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:4237:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:3666:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:3677:28: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:3696:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:3330:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:3341:28: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:3360:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:3036:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:3047:28: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:3066:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:2836:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:2847:28: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:2866:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:27917:17: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:18011:33: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:18011:33: optimized:  loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:18011:33: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:18011:33: optimized:  loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:18011:33: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:18011:33: optimized:  loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:18011:33: optimized: loop vectorized using 32 byte vectors\n",
      "popcount.c:18011:33: optimized:  loop versioned for vectorization because of possible aliasing\n",
      "popcount.c:15121:3: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:23525:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:10383:5: optimized: basic block part vectorized using 16 byte vectors\n",
      "popcount.c:10422:19: optimized: basic block part vectorized using 16 byte vectors\n"
     ]
    }
   ],
   "source": [
    "# Note: -O3 -march=native isn't just for fun, you really do need it to beat NumPy.\n",
    "! which cython || conda install -y cython\n",
    "! test ! -d $$CONDA_PREFIX/envs/myenv/lib/python3.6/site-packages/blosc || conda install -y python-blosc\n",
    "! cython popcount.pyx\n",
    "! sed -i '/0xbad0bad0/d' popcount.c\n",
    "#! g++ -Wall -O3 -Ofast -g -flto -lm -L$$CONDA_PREFIX/lib -lblosc -shared -pthread -fPIC -funroll-loops -fno-strict-aliasing -march=native -mno-avx256-split-unaligned-load -fopt-info-vec-optimized -I$$CONDA_PREFIX/include/python3.6m -I$$CONDA_PREFIX/include -I$$CONDA_PREFIX/lib/python3.6/site-packages/numpy/core/include/ -o popcount.so popcount.c\n",
    "! g++ -Wall -O3 -Ofast -g -flto -fPIC -funroll-loops -fopenmp -fwrapv -fno-strict-aliasing -march=native -mno-avx256-split-unaligned-load -fopt-info-vec-optimized -I$$CONDA_PREFIX/include/python3.6m -I$$CONDA_PREFIX/include -I$$CONDA_PREFIX/lib/python3.6/site-packages/numpy/core/include -c -o popcount.o popcount.c\n",
    "! g++ -Wall -O3 -Ofast -g -flto -shared -pthread -fPIC -funroll-loops -fopenmp -fwrapv -fno-strict-aliasing -march=native -mno-avx256-split-unaligned-load -fopt-info-vec-optimized -mno-avx256-split-unaligned-load -o popcount.so popcount.o -L$$CONDA_PREFIX/lib -Wl,-rpath=$$CONDA_PREFIX/lib -lblosc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy packed bits popcounted\" took 696.1603999952786 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import inplace_popcount_32\n",
    "with Timer(\"numpy packed bits popcounted\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    isct = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1) & np.packbits(query_f), dtype=np.uint32)\n",
    "    inplace_popcount_32(isct)\n",
    "    scores = np.sum(isct.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're processing more than 5x faster than it takes to read the database off the SSD, we can declare victory, but also I'm curious how far we can go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing operations\n",
    "This is sort of a diminishing returns scenario; I needed `-O3 -march=native` to run the bitwise and faster than NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numpy fused bitwise_and + popcount\" took 779.3745000017225 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and\n",
    "with Timer(\"numpy fused bitwise_and + popcount\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "    fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing the packing too\n",
    "The packbits function is now the slowest part, at about 80% of the runtime. What if we don't do that at all?\n",
    "\n",
    "It ends up being slower than NumPy because NumPy is [using proper SIMD instructions for this](https://github.com/numpy/numpy/blob/97d2db483fc0ffd46f38d0e1c39d5fc001e33197/numpy/core/src/multiarray/compiled_base.c#L1543). This is okay, we now have the tools to do this properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"fused notpacked popcount\" took 1699.7953000027337 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and_notpacked_count\n",
    "with Timer(\"fused notpacked popcount\", 1000000):\n",
    "    query_f = compute_fingerprint(query).astype(np.uint8)\n",
    "    fused_popcount_bitwise_and_notpacked_count(query_f, fingerprints[:1000000])\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endgame: AVX2\n",
    "AVX2 is supported by most modern processors, including the Intel processor that the specified MBP 2019 has.\n",
    "\n",
    "We get an absurdly high throughput: we're processing about 5 boolean database entries per nanosecond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"avx2\" took 166.72290000133216 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_avx2\n",
    "with Timer(\"avx2\", 1000000):\n",
    "    query_f = compute_fingerprint(query).astype(np.uint8)\n",
    "    scores = fused_popcount_avx2(query_f, fingerprints[:1000000]) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments and things to improve\n",
    "\n",
    "Algorithmic complexity is O(database size), since it must iterate over all the booleans in every row. I'm fairly certain that this cannot be asymptotically improved - for example in the worst case every molecule in the database contains all substructures (all 2048 bits are true).\n",
    "\n",
    "Memory usage is high; we're reading the entire database into memory and keeping it there. There isn't really any way around keeping O(database size) in memory because SSD bandwidth is far too low to support streaming from disk, but an 8x memory usage/bandwidth bottleneck improvement can probably be found by bitpacking the database beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Overtime stuff\n",
    "\n",
    "This was done for fun significantly over the 6hr time limit; please don't count this if you're evaluating quantity of code produced.\n",
    "\n",
    "## Pre-packing\n",
    "\n",
    "I did no memory-related optimizations mostly because I was more interested in playing with AVX, and because up until AVX the whole thing was compute-bottlenecked rather than memory-bottlenecked. However it's trivial to do so; just don't count the bitpacking step from the `Fusing operations` section, and you get (only!) a slight speedup over the AVX implementation as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 247.14639999729113 nanoseconds per db row\n",
      "\"pre-packed rows\" took 134.66370000242023 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "with Timer(\"pre-packed rows\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "    fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVX2 popcount attempt 1\n",
    "\n",
    "Not working correctly, but AVX2 emulated popcnt [can be 30% faster](https://stackoverflow.com/a/50082218) than the dedicated instruction.\n",
    "\n",
    "Edit: I think it won't be faster; according to [this paper](https://arxiv.org/pdf/1611.07612.pdf) and other sources, popcnt on 64-bit ints is the fastest way to do it at our data size of 2048 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 254.77520000276854 nanoseconds per db row\n",
      "\"pre-packed rows\" took 67.35289999778615 nanoseconds per db row\n",
      "[4.03833063e+15 2.62815452e+16 3.77945367e+15 3.34960946e+16\n",
      " 8.88579474e+15]\n",
      "[439306  28215  93292 489702 106223] [3.83505627e+16 3.83506231e+16 3.83505872e+16 3.83507498e+16\n",
      " 3.83504301e+16]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_avx2_emulated_popcount\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint64)\n",
    "    fused_avx2_emulated_popcount(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcount64 and some tweaks to get good assembly\n",
    "\n",
    "Let's use popcount64 directly on 64-bit ints, instead of 32-bit ints, and also do counting in the kernel itself. (Also added loop unrolling flag, for the slight benefit of all these examples.)\n",
    "\n",
    "Maybe let's be less guessy. Here's a relevant assembly sample from a single iteration of the unrolled loop.\n",
    "```\n",
    "   27c1a:       44 09 c2                or     %r8d,%edx\n",
    "   27c1d:       41 89 07                mov    %eax,(%r15)\n",
    "   27c20:       48 63 ca                movslq %edx,%rcx\n",
    "   27c23:       49 0f af cc             imul   %r12,%rcx\n",
    "   27c27:       49 8b 14 0b             mov    (%r11,%rcx,1),%rdx\n",
    "   27c2b:       48 23 17                and    (%rdi),%rdx\n",
    "   27c2e:       31 c9                   xor    %ecx,%ecx\n",
    "   27c30:       4c 01 cf                add    %r9,%rdi\n",
    "   27c33:       f3 48 0f b8 ca          popcnt %rdx,%rcx\n",
    "   27c38:       8d 56 02                lea    0x2(%rsi),%edx\n",
    "   27c3b:       01 c8                   add    %ecx,%eax\n",
    "```\n",
    "There might be a tiny bit of gain to be had by avoiding some of these instructions. I don't fully understand what they do, though.\n",
    "\n",
    "Making `count` as a separate int variable (instead of accumulating directly into `counts[i>>5]`) results in much less consistent loop unrolling - there doesn't actually seem to be a particular pattern to the iterations; they have a bunch of random instructions squeezed in between that don't necessarily repeat. Finally, making `fingerprints_packed_curr` as a temporary variable instead of calculating `i|j` every time makes the loop much tighter:\n",
    "```\n",
    "   27d17:       4c 8b 79 d0             mov    -0x30(%rcx),%r15    ; Get a quadword of fingerprints_packed_curr (fixed offset)\n",
    "   27d1b:       4c 23 3a                and    (%rdx),%r15         ; AND it with a query quadword\n",
    "   27d1e:       48 01 f2                add    %rsi,%rdx           ; Advance the query pointer\n",
    "   27d21:       f3 4d 0f b8 ff          popcnt %r15,%r15           ; Popcount it\n",
    "   27d26:       41 01 c7                add    %eax,%r15d          ; Add this to the count\n",
    "```\n",
    "Those two modifications combined give an additional 25-30% better performance. I don't think there's anything else that can be done to speed this up, unless AVX2 has some real magic to offer (but even then, the frequency hit from AVX/AVX2 probably means that it won't be able to overcome this).\n",
    "\n",
    "The fastest I've seen for just the `fused_popcount64_bitwise_and` part is 30ns:\n",
    "- 2048bits/30ns = 8.5 GBps, out of a memory bandwidth of ~10-20GBps.\n",
    "- 32 popcounts / 30ns = approx one 64-bit popcount per 0.9 nanoseconds. This almost double the expected CPI of 5 cycles @ 3GHz = 1.6 ns, assuming the 5 listed instructions above take 1 cycle each, but apparently this is very superscalar, or my math is wrong (is frequency == cycles per second?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 380.1510999983293 nanoseconds per db row\n",
      "\"pre-packed rows\" took 35.255220000544796 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='little'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows\", 10000000):\n",
    "    for i in range(10):\n",
    "        query_f = compute_fingerprint(query)\n",
    "        query_packed = np.frombuffer(np.packbits(query_f, bitorder='little'), dtype=np.uint64)\n",
    "        counts = fused_popcount64_bitwise_and(query_packed, fingerprints_packed)\n",
    "        total = fused_popcount64_bitwise_and(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "        topk = np.argpartition(-counts, k)[:k]\n",
    "print(counts[:k]/total)\n",
    "print(topk, counts[topk]/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVX2 attempt 2: success and maximum throughput\n",
    "\n",
    "It might be possible to use AVX2 instructions to MOV or AND or ADD more efficiently; 10ns is the absolute limit because the popcount instruction has a CPI of 1. I'm highly doubtful it'll make things faster given that the width is only 256 bits anyway though, and there has to be some penalty moving between AVX registers and the regular registers where popcnt lives, though maybe that can be pipelined away.\n",
    "- LOADU twice for 256 bits each\n",
    "- AND 256 bits together\n",
    "- _mm256_extract_epi64 and popcount each of the 4 registers, adding\n",
    "\n",
    "With some guidance from [this](https://github.com/WojciechMula/sse-popcount/blob/master/popcnt-avx2-lookup.cpp) we can get a 20% speed boost.\n",
    "\n",
    "It turns out that even though benchmarks show unrolled loop of int64 popcnt is the fastest popcount implementation for 256 bytes, for our usecase we have additional math alongside it (the AND) so putting everything into AVX is actually beneficial.\n",
    "\n",
    "Our inner loop looks something like (\"something like\" because loop unrolling has made things confusing) this:\n",
    "```\n",
    "   28b23:       c4 43 25 38 6d 10 01    vinserti128 $0x1,0x10(%r13),%ymm11,%ymm13\n",
    "   28b2a:       c5 d5 71 d4 04          vpsrlw $0x4,%ymm4,%ymm5\n",
    "   28b2f:       c4 63 35 38 54 07 50    vinserti128 $0x1,0x50(%rdi,%rax,1),%ymm9,%ymm10\n",
    "   28b36:       01 \n",
    "   28b37:       c4 c1 7d fc c7          vpaddb %ymm15,%ymm0,%ymm0\n",
    "   28b3c:       c5 6d db c5             vpand  %ymm5,%ymm2,%ymm8\n",
    "   28b40:       c5 dd db f2             vpand  %ymm2,%ymm4,%ymm6\n",
    "   28b44:       c4 c1 15 db ca          vpand  %ymm10,%ymm13,%ymm1\n",
    "   28b49:       c4 42 1d 00 f0          vpshufb %ymm8,%ymm12,%ymm14\n",
    "   28b4e:       c4 41 7a 6f 14 24       vmovdqu (%r12),%xmm10\n",
    "   28b54:       c4 e2 1d 00 fe          vpshufb %ymm6,%ymm12,%ymm7\n",
    "   28b59:       c5 7a 6f 44 07 60       vmovdqu 0x60(%rdi,%rax,1),%xmm8\n",
    "```\n",
    "Even though it's longer, it's also handling 4x as much data per iteration.\n",
    "\n",
    "Turning on `-march=native -mno-avx256-split-unaligned-load` gets us to something like\n",
    "```\n",
    "   289ac:       c4 41 45 db cd          vpand  %ymm13,%ymm7,%ymm9\n",
    "   289b1:       c5 7d fc fe             vpaddb %ymm6,%ymm0,%ymm15\n",
    "   289b5:       c5 ed db bc 07 a0 00    vpand  0xa0(%rdi,%rax,1),%ymm2,%ymm7\n",
    "   289bc:       00 00 \n",
    "   289be:       c4 c1 05 fc f6          vpaddb %ymm14,%ymm15,%ymm6\n",
    "   289c3:       c4 e2 1d 00 e1          vpshufb %ymm1,%ymm12,%ymm4\n",
    "   289c8:       c4 c1 15 db c8          vpand  %ymm8,%ymm13,%ymm1\n",
    "   289cd:       c4 c2 1d 00 d9          vpshufb %ymm9,%ymm12,%ymm3\n",
    "   289d2:       c5 bd 71 d7 04          vpsrlw $0x4,%ymm7,%ymm8\n",
    "```\n",
    "Where we only see instructions that we've specified explicitly in the code. In fact, we're missing a bunch of `vmovdqu`'s - the compiler is doing something clever here that I don't understand.\n",
    "\n",
    "- 2048bits/22ns = 11.6 GBps, out of a memory bandwidth of ~10-20GBps.\n",
    "- ~80 AVX instructions / 22ns = approx 3.6GHz, which is significantly higher than the clock frequency. This is probably because some instructions can have a throughput of more than 1 per cycle.\n",
    "\n",
    "This may well be the end of the line for optimizing this, as this does seem to be optimal (all sources point to Mula as the source of these algorithms, and this is the best his papers have to offer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 270.2156999948784 nanoseconds per db row\n",
      "\"pre-packed rows 1\" took 1.0187100000621285 nanoseconds per db row\n",
      "\"pre-packed rows 2\" took 0.01899999988381751 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 19.55549999984214 nanoseconds per db row\n",
      "\"pre-packed rows 4\" took 0.004130000161239877 nanoseconds per db row\n",
      "\"pre-packed rows 5\" took 4.757419999805279 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='little'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 1\", 10000000):\n",
    "    for i in range(10):\n",
    "        query_f = compute_fingerprint(query)\n",
    "with Timer(\"pre-packed rows 2\", 10000000):\n",
    "    for i in range(10):\n",
    "        query_packed = np.frombuffer(np.packbits(query_f, bitorder='little'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 3\", 10000000):\n",
    "    for i in range(10):\n",
    "        counts = fused_popcount64_bitwise_and_avx(query_packed, fingerprints_packed)\n",
    "with Timer(\"pre-packed rows 4\", 10000000):\n",
    "    for i in range(10):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "with Timer(\"pre-packed rows 5\", 10000000):\n",
    "    for i in range(10):\n",
    "        topk = np.argpartition(-counts, k)[:k]\n",
    "print(counts[:k]/total)\n",
    "print(topk, counts[topk]/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finale: Optimizing other parts of the system\n",
    "\n",
    "We're actually getting to the point where the final np.argpartition on 1 million elements is taking about 25% of the total time. Using np.argsort is 10x slower. We did actually need this initially seemingly unnecessary optimization!\n",
    "\n",
    "There are [ways](https://github.com/WojciechMula/simd-sort) to make this happen faster, but since we're going to be generating the entire count array anyway, why not just construct it on the fly? Specifically, let's use a **min-heap of size k** to track the k largest elements we've seen.\n",
    "\n",
    "This absorbs the topk algorithm without any measurable performance penalty (less than 10% with k=1000). A bit of tweaking suffices to double the speed of the compute_fingerprint, and now we are truly at the end of the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 273.9852999948198 nanoseconds per db row\n",
      "\"pre-packed rows 0\" took 0.3653179999673739 nanoseconds per db row\n",
      "\"pre-packed rows 1\" took 0.4333799999585608 nanoseconds per db row\n",
      "\"pre-packed rows 2\" took 0.039453000063076615 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 19.410000000789296 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 25.731800000357907 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 21.555900006205775 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 24.17089999653399 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 22.931200001039542 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 26.536099998338614 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 23.517900001024827 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 20.306099999288563 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 19.850900003802963 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 24.153399994247593 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.91160000336822 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.29870000074152 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.695000005711336 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.89820000360487 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.854900000907946 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.90270000108285 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.364399998565204 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.420100000686944 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 22.20569999917643 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.946600004506763 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.740399998729117 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.42080000601709 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.783100000291597 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 23.223999996844213 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 22.120000001450535 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 21.309100004145876 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.852599994512275 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.00239999627229 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.643299997667782 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.677199993864633 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.877700003737118 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.439599999808706 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.660000001138542 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.36819999839645 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.079300003591925 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.456500001368113 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.191299997852184 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.928599998005666 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.14080000622198 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.549899999517947 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.4808999945526 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.793499995197635 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.882199997780845 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.49110000289511 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.390000000887085 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.218400000070687 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.260000000533182 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.134699995745905 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.163100000994746 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.680500000598842 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.16659999510739 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.34419999754755 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.455100005259737 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.909100002318157 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.349300000001676 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.56899999943562 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.437000005680602 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.164500000944827 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.498600005637854 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.204500003368594 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.77709999785293 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.772200000763405 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.90279999456834 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.670899996825028 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.81799999641953 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.88350000040373 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.189500002132263 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.002000004344154 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.834900001820643 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 23.460399999748915 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 20.416999999724794 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.82369999657385 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 21.156300004804507 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 21.0949000029359 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 24.53840000089258 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 21.87959999719169 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 31.01829999650363 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 31.962900000507943 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 21.621599997160956 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 20.71330000035232 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 22.54720000200905 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 24.49440000054892 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 38.933099996938836 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 23.35279999533668 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 25.587099997210316 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 24.095299995678943 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 24.721199995838106 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 20.944700001564343 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 20.741499996802304 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 22.950699996727053 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 21.33470000262605 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 19.38240000163205 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 22.318000002996996 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 20.459500003198627 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 24.975100001029205 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 24.93030000186991 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 19.596500002080575 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.577499998675194 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.76429999881657 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.879400005564094 nanoseconds per db row\n",
      "\"pre-packed rows 4\" took 0.0023800000053597614 nanoseconds per db row\n",
      "[324758 329123 324807 324852 329172] [0.66735967 0.66735967 0.66735967 0.66943867 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx_topk, fused_popcount64_bitwise_and_avx\n",
    "from rdkit import Chem\n",
    "\n",
    "import os\n",
    "# This only works before the first run of openmp in a process\n",
    "os.environ[\"OMP_PLACES\"] = \"cores\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='big'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 0\", 100000000):\n",
    "    for i in range(100):\n",
    "        mol = Chem.MolFromSmiles(query)\n",
    "with Timer(\"pre-packed rows 1\", 100000000):\n",
    "    for i in range(100):\n",
    "        s = Chem.RDKFingerprint(mol, fpSize=2048, maxPath=5).ToBitString()\n",
    "with Timer(\"pre-packed rows 2\", 100000000):\n",
    "    for i in range(100):\n",
    "        query_packed = np.copy(np.frombuffer(int(s, 2).to_bytes(len(s) // 8, byteorder='big'), dtype=np.uint64))\n",
    "for i in range(100):\n",
    "    with Timer(\"pre-packed rows 3\", 1000000):\n",
    "        counts, topk = fused_popcount64_bitwise_and_avx_topk(query_packed, fingerprints_packed, k)\n",
    "with Timer(\"pre-packed rows 4\", 100000000):\n",
    "    for i in range(100):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "print(topk, counts/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenMP Threading\n",
    "\n",
    "After some OpenMP we can smash the 20ns barrier.\n",
    "\n",
    "\"personal best\" = \"5.8700999999814485 nanoseconds per db row\" for `fused_popcount64_bitwise_and_avx_topk`\n",
    "\n",
    "3600MHz RAM, 2048 bits / 5.869199999978036 ns = 43.6175288 GBps, after moving to my Ryzen 3700x machine which should have a max of 28.8GBps per stick, times two sticks.\n",
    "\n",
    "Going from 2133MHz to 3600MHz speeds up by 50%, and overclocking CPU doesn't visibly help, so it's legitimately memory bandwidth bound, and even after the frequency increase to 3600MHz it's still probably memory bound. That's pretty wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 247.32909999875122 nanoseconds per db row\n",
      "\"pre-packed rows 0\" took 0.2136400000017602 nanoseconds per db row\n",
      "\"pre-packed rows 1\" took 0.31977300001017284 nanoseconds per db row\n",
      "\"pre-packed rows 2\" took 0.023954000062076375 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.315223999990849 nanoseconds per db row\n",
      "\"pre-packed rows 4\" took 0.0026619999698596075 nanoseconds per db row\n",
      "[324758 329123 324807 324852 329172] [0.66735967 0.66735967 0.66735967 0.66943867 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx_topk_omp, fused_popcount64_bitwise_and_avx\n",
    "from rdkit import Chem\n",
    "\n",
    "import os\n",
    "# This only works before the first run of openmp in a process\n",
    "os.environ[\"OMP_PLACES\"] = \"cores\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='big'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 0\", 100000000):\n",
    "    for i in range(100):\n",
    "        mol = Chem.MolFromSmiles(query)\n",
    "with Timer(\"pre-packed rows 1\", 100000000):\n",
    "    for i in range(100):\n",
    "        s = Chem.RDKFingerprint(mol, fpSize=2048, maxPath=5).ToBitString()\n",
    "with Timer(\"pre-packed rows 2\", 100000000):\n",
    "    for i in range(100):\n",
    "        query_packed = np.copy(np.frombuffer(int(s, 2).to_bytes(len(s) // 8, byteorder='big'), dtype=np.uint64))\n",
    "with Timer(\"pre-packed rows 3\", 100000000):\n",
    "    for i in range(100):\n",
    "        counts, topk = fused_popcount64_bitwise_and_avx_topk_omp(query_packed, fingerprints_packed, k)\n",
    "with Timer(\"pre-packed rows 4\", 100000000):\n",
    "    for i in range(100):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "print(topk, counts/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed: blosc\n",
    "Since we might have a memory bottleneck, does blosc help?\n",
    "\n",
    "Nope, that was a total failure. Maybe next step is to try out custom encodings like RLE or even sparse bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression: 0.70317598828125\n"
     ]
    }
   ],
   "source": [
    "import blosc\n",
    "fingerprints_blosc_compressed = np.frombuffer(blosc.compress(fingerprints_packed, typesize=8, shuffle=blosc.NOSHUFFLE, cname='blosclz'), dtype=np.uint8)\n",
    "\n",
    "print(\"Compression:\", len(fingerprints_blosc_compressed)/8/len(fingerprints_packed))\n",
    "# %timeit blosc.decompress(fingerprints_blosc_compressed)\n",
    "# %timeit np.copy(fingerprints_packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 566.3571000040974 nanoseconds per db row\n",
      "\"pre-packed rows 0\" took 0.3708000003825873 nanoseconds per db row\n",
      "\"pre-packed rows 1\" took 0.38779999886173755 nanoseconds per db row\n",
      "\"pre-packed rows 2\" took 0.05509999755304307 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 57.723399993847124 nanoseconds per db row\n",
      "\"pre-packed rows 4\" took 0.03330000618007034 nanoseconds per db row\n",
      "[905932 324808 324758 324807 324852] [0.64241164 0.66320166 0.66735967 0.66735967 0.66943867]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx_topk_omp_blosc, fused_popcount64_bitwise_and_avx\n",
    "from rdkit import Chem\n",
    "import blosc\n",
    "\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints, axis=1, bitorder='big'), dtype=np.uint64)\n",
    "    fingerprints_blosc_compressed = np.frombuffer(blosc.compress(fingerprints_packed, typesize=8, shuffle=blosc.NOSHUFFLE, cname='blosclz'), dtype=np.uint8)\n",
    "    n_fingerprints = len(fingerprints)\n",
    "with Timer(\"pre-packed rows 0\", 1000000):\n",
    "    for i in range(1):\n",
    "        mol = Chem.MolFromSmiles(query)\n",
    "with Timer(\"pre-packed rows 1\", 1000000):\n",
    "    for i in range(1):\n",
    "        s = Chem.RDKFingerprint(mol, fpSize=2048, maxPath=5).ToBitString()\n",
    "with Timer(\"pre-packed rows 2\", 1000000):\n",
    "    for i in range(1):\n",
    "        query_packed = np.copy(np.frombuffer(int(s, 2).to_bytes(len(s) // 8, byteorder='big'), dtype=np.uint64))\n",
    "with Timer(\"pre-packed rows 3\", 1000000):\n",
    "    for i in range(1):\n",
    "        counts, topk = fused_popcount64_bitwise_and_avx_topk_omp_blosc(query_packed, fingerprints_blosc_compressed, k, n_fingerprints)\n",
    "with Timer(\"pre-packed rows 4\", 1000000):\n",
    "    for i in range(1):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "print(topk, counts/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future stuff\n",
    "AVX-512 would be fun but I don't have ice lake hardware sadly. This should be something like 3x faster, less any AVX-induced processor frequency hit.\n",
    "- `_mm512_loadu_epi64`\n",
    "- `_mm512_and_epi64`\n",
    "- `_mm512_popcnt_epi64`\n",
    "- `_mm512_reduce_add_epi64`\n",
    "\n",
    "Some additional cleverness that I do not have time to explore, but which is unlikely to yield better results:\n",
    "- Database rows are almost always sparse. There might be more efficient popcount algorithms in this case.\n",
    "- OpenCL was originally planned but no time, and also I think my laptop iGPU doesn't support it anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Threading\n",
    "Because why not? The AVX2 code is mostly in C-land away from the GIL so we might be able to get close to linear speedup in the number of cores (regular cores; hyperthreading probably does not help for this because AVX2 resources are shared).\n",
    "\n",
    "This didn't help at all with the AVX2 code because there are other bottlenecks e.g. system memory bandwidth. 2GB dataset in 0.2s is 10GBps, which is around half the theoretical memory bandwidth my machine could have, but there's probably some other caveats there.\n",
    "\n",
    "This does help a lot (+50% ish) with regular popcount64 because of the 8x lower memory bandwidth requirement when bits are packed. 2GB/8 = 256MB dataset in 0.08s is only 3GBps, so it's probably still compute limited and could benefit from more threading. Unfortunately I only have 2 actual cores (4 hyperthreaded) so there can be no more than a 2x speedup.\n",
    "\n",
    "However modifying popcount64 to fuse the counting step as well results in having no significant threading benefit. Occasionally it will run faster than single-threaded, but most of the time it's running into some issue or another - maybe memory bandwidth, maybe scheduler issues because it's so fast.\n",
    "\n",
    "This also points toward there being very little point in using OpenCL: unless you have a discrete GPU with dedicated high bandwidth video memory, which isn't the case on MBP 2019, it's not going to get you much further, and more likely will slow things down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pre-packing process\" took 348.66300000430783 nanoseconds per db row\n",
      "Using 8 threads\n",
      "\"threaded popcount64\" took 30.186699994374067 nanoseconds per db row\n",
      "[324852 329123 324807 329172 324758] [0.66943867 0.66735967 0.66735967 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from popcount import fused_popcount64_bitwise_and\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='little'), dtype=np.uint64)\n",
    "\n",
    "core_ids = set()\n",
    "with open('/proc/cpuinfo') as f:\n",
    "    for line in f:\n",
    "        if line.count('core id'):\n",
    "            core_ids.add(line)\n",
    "\n",
    "nthreads = len(core_ids)\n",
    "assert(len(fingerprints) % nthreads == 0) # Note: below code calculating start and end assumes that nthreads divides len(fingerprints)\n",
    "print(f\"Using {nthreads} threads\")\n",
    "\n",
    "topk_per_thread = [None] * nthreads\n",
    "topk_per_thread_scores = [None] * nthreads\n",
    "\n",
    "def thread_func(query_packed, threadid):\n",
    "    start = threadid*len(fingerprints_packed)//nthreads\n",
    "    end = (threadid+1)*len(fingerprints_packed)//nthreads\n",
    "    \n",
    "    counts = fused_popcount64_bitwise_and(query_packed, fingerprints_packed[start:end])\n",
    "    total = fused_popcount64_bitwise_and(query_packed, query_packed)\n",
    "\n",
    "    topk_per_thread[threadid] = np.argpartition(-counts, k)[:k]\n",
    "\n",
    "    topk_per_thread_scores[threadid] = counts[topk_per_thread[threadid]] / total\n",
    "    topk_per_thread[threadid] += start // 32\n",
    "\n",
    "with Timer(\"threaded popcount64\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f, bitorder='little'), dtype=np.uint64)\n",
    "    threads = [None] * nthreads\n",
    "    for i in range(1, nthreads):\n",
    "        threads[i] = threading.Thread(target=thread_func, args=(query_packed, i))\n",
    "        threads[i].start()\n",
    "    thread_func(query_packed, 0)\n",
    "    for i in range(1, nthreads):\n",
    "        threads[i].join()\n",
    "    topk_arg = np.argsort(-np.concatenate(topk_per_thread_scores))[:k]\n",
    "    scores = np.concatenate(topk_per_thread_scores)[topk_arg]\n",
    "    topk = np.concatenate(topk_per_thread)[topk_arg]\n",
    "print(topk, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: A failed attempt at using Numba\n",
    "Cython has some ... interoperability issues ... with Numba, so using the popcount kernel isn't possible.\n",
    "\n",
    "Using purely Numba (without the popcount kernel) is way slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from numba import jit\n",
    "except ImportError:\n",
    "    ! conda install -y numba\n",
    "    from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"numba\" took 44663.81699996418 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[94115 39023 94179 94185 38955] [0.60914761 0.59459459 0.6029106  0.6008316  0.5966736 ]\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "# from numba.extending import get_cython_function_address\n",
    "# import numpy.ctypeslib as npct\n",
    "# import ctypes\n",
    "\n",
    "# array_1d_uint32 = npct.ndpointer(dtype=np.uint32, ndim=1, flags='CONTIGUOUS')\n",
    "# addr = get_cython_function_address(\"popcount\", \"_fused_popcount_bitwise_and\")\n",
    "# functype = ctypes.CFUNCTYPE(None, array_1d_uint32, array_1d_uint32)\n",
    "# fused_popcount_bitwise_and = functype(addr)\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def numba_func(query_f, fingerprints):\n",
    "    return np.sum(query_f & fingerprints[:100000], axis=1) / np.sum(query_f)\n",
    "\n",
    "# @numba.jit(nopython=True)\n",
    "# def numba_func(query_f, fingerprints):\n",
    "#     query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "#     fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "#     fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "#     return np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "\n",
    "with Timer(\"numba\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = numba_func(query_f, fingerprints)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import popcount\n",
    "popcount = importlib.reload(popcount)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
