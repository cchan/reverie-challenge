{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class Timer:\n",
    "    def __init__(self, s, iters):\n",
    "        self.s = s\n",
    "        self.iters = iters\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        print(\"\\\"\" + self.s + \"\\\"\", \"took\", (time.perf_counter() - self.start)/self.iters*1e9, \"nanoseconds per db row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"db loading\" took 1763.2585000101244 nanoseconds per db row\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from starter import compute_fingerprint, load_database\n",
    "\n",
    "with Timer(\"db loading\", 1000000):\n",
    "    fingerprints = load_database(\"database_fingerprints.npy\")\n",
    "    molecules = pd.read_csv(\"database.csv\")\n",
    "with open(\"query.txt\") as q:\n",
    "  query = q.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup\n",
    "Just python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"python coverage function\" took 2271196.100002271 nanoseconds per db row\n[0.2515592515592516, 0.22245322245322247, 0.2785862785862786, 0.22453222453222454, 0.30353430353430355]\n[928  66  59 429 428] [0.4261954261954262, 0.4303534303534304, 0.4365904365904366, 0.46361746361746364, 0.498960498960499]\n"
     ]
    }
   ],
   "source": [
    "def coverage(query, ref):\n",
    "  query_f = compute_fingerprint(query)\n",
    "  ref_f = compute_fingerprint(ref)\n",
    "  return sum(query_f & ref_f) / sum(query_f)\n",
    "\n",
    "with Timer(\"python coverage function\", 1000):\n",
    "    scores = [coverage(query, molecules[\"smiles\"][i]) for i in range(1000)]\n",
    "    topk = np.argsort(scores)[-k:]\n",
    "print(scores[:k])\n",
    "print(topk, [scores[i] for i in topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some sanity checks before we proceed\n",
    "After we check that these scores are meaningful, we can proceed and just verify that our scores are exactly identical to this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n0.746268656716418\n0.0\n"
     ]
    }
   ],
   "source": [
    "print(coverage(molecules[\"smiles\"][0], molecules[\"smiles\"][0])) # should be 1\n",
    "print(coverage(molecules[\"smiles\"][0], molecules[\"smiles\"][1])) # should be between 0 and 1\n",
    "print(coverage(\"c1ccccc1\", \"C1CCCCC1\")) # should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed things up with NumPy\n",
    "We can get a massive speedup by using NumPy; everything is automatically vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"numpy direct bitwise_and\" took 6549.891000031494 nanoseconds per db row\n[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n[94115 39023 94179 94185 38955] [0.60914761 0.59459459 0.6029106  0.6008316  0.5966736 ]\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"numpy direct bitwise_and\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = np.sum(query_f & fingerprints[:100000], axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packing bits\n",
    "Packing bits allows us to `&` 8 bits at a time.\n",
    "\n",
    "Unpacking and summing is extremely inefficient though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"numpy packed bits bitwise_and\" took 1967.934999993304 nanoseconds per db row\n[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n[94115 39023 94179 94185 38955] [0.60914761 0.59459459 0.6029106  0.6008316  0.5966736 ]\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"numpy packed bits bitwise_and\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = np.sum(np.unpackbits(np.packbits(fingerprints[:100000], axis=1) & np.packbits(query_f)).reshape(-1, 2048), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython\n",
    "We need C++ to get even faster; I want to use the native popcount instruction because [NumPy doesn't have a function to do that yet](https://github.com/numpy/numpy/issues/16325)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/clive/anaconda3/envs/my-rdkit-env/bin/cython\n",
      "AMD clang version 11.0.0 (CLANG: AOCC_2.3.0-Build#85 2020_11_10) (based on LLVM Mirror.Version.11.0.0)\n",
      "Target: x86_64-unknown-linux-gnu\n",
      "Thread model: posix\n",
      "InstalledDir: /opt/AMD/aocc-compiler-2.3.0/bin\n",
      "clang-11: \u001b[0;1;35mwarning: \u001b[0mtreating 'c' input as 'c++' when in C++ mode, this behavior is deprecated [-Wdeprecated]\u001b[0m\n",
      "In file included from popcount.c:610:\n",
      "In file included from /home/clive/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4:\n",
      "In file included from /home/clive/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12:\n",
      "In file included from /home/clive/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1822:\n",
      "\u001b[1m/home/clive/anaconda3/envs/my-rdkit-env/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m\"Using deprecated NumPy API, disable it with \"          \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-W#warnings]\u001b[0m\n",
      "#warning \"Using deprecated NumPy API, disable it with \" \\\n",
      "\u001b[0;1;32m ^\n",
      "\u001b[0m\u001b[1mpopcount.c:5822:18: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mequality comparison with extraneous parentheses [-Wparentheses-equality]\u001b[0m\n",
      "  if ((__pyx_t_3 == 0)) abort();\n",
      "\u001b[0;1;32m       ~~~~~~~~~~^~~~\n",
      "\u001b[0m\u001b[1mpopcount.c:5822:18: \u001b[0m\u001b[0;1;30mnote: \u001b[0mremove extraneous parentheses around the comparison to silence this warning\u001b[0m\n",
      "  if ((__pyx_t_3 == 0)) abort();\n",
      "\u001b[0;1;32m      ~          ^   ~\n",
      "\u001b[0m\u001b[1mpopcount.c:5822:18: \u001b[0m\u001b[0;1;30mnote: \u001b[0muse '=' to turn this equality comparison into an assignment\u001b[0m\n",
      "  if ((__pyx_t_3 == 0)) abort();\n",
      "\u001b[0;1;32m                 ^~\n",
      "\u001b[0m\u001b[0;32m                 =\n",
      "\u001b[0m^C\n"
     ]
    }
   ],
   "source": [
    "# Note: -O3 -march=native isn't just for fun, you really do need it to beat NumPy.\n",
    "! which cython || conda install -y cython\n",
    "! test ! -d $$CONDA_PREFIX/envs/myenv/lib/python3.6/site-packages/blosc || conda install -y python-blosc\n",
    "! cython popcount.pyx\n",
    "! sed -i '/0xbad0bad0/d' popcount.c\n",
    "CXX=\"/opt/AMD/aocc-compiler-2.3.0/bin/clang++\"\n",
    "! $CXX --version\n",
    "#! g++ -Wall -O3 -Ofast -g -flto -lm -L$$CONDA_PREFIX/lib -lblosc -shared -pthread -fPIC -funroll-loops -fno-strict-aliasing -march=native -mno-avx256-split-unaligned-load -fopt-info-vec-optimized -I$$CONDA_PREFIX/include/python3.6m -I$$CONDA_PREFIX/include -I$$CONDA_PREFIX/lib/python3.6/site-packages/numpy/core/include/ -o popcount.so popcount.c\n",
    "! $CXX -Wall -Ofast -g -flto -fPIC -funroll-loops -fopenmp -fwrapv -fno-strict-aliasing -march=native -I$$CONDA_PREFIX/include/python3.7m -I$$CONDA_PREFIX/include -I$$CONDA_PREFIX/lib/python3.7/site-packages/numpy/core/include -c -o popcount.o popcount.c\n",
    "! $CXX -Wall -Ofast -g -flto -shared -pthread -fPIC -funroll-loops -fopenmp -fwrapv -fno-strict-aliasing -march=native -o popcount.so popcount.o -L$$CONDA_PREFIX/lib -Wl,-rpath=$$CONDA_PREFIX/lib -lblosc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"numpy packed bits popcounted\" took 418.0770999955712 nanoseconds per db row\n[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import inplace_popcount_32\n",
    "with Timer(\"numpy packed bits popcounted\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    isct = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1) & np.packbits(query_f), dtype=np.uint32)\n",
    "    inplace_popcount_32(isct)\n",
    "    scores = np.sum(isct.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're processing more than 5x faster than it takes to read the database off the SSD, we can declare victory, but also I'm curious how far we can go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing operations\n",
    "This is sort of a diminishing returns scenario; I needed `-O3 -march=native` to run the bitwise and faster than NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"numpy fused bitwise_and + popcount\" took 349.76969999843277 nanoseconds per db row\n[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and\n",
    "with Timer(\"numpy fused bitwise_and + popcount\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "    fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing the packing too\n",
    "The packbits function is now the slowest part, at about 80% of the runtime. What if we don't do that at all?\n",
    "\n",
    "It ends up being slower than NumPy because NumPy is [using proper SIMD instructions for this](https://github.com/numpy/numpy/blob/97d2db483fc0ffd46f38d0e1c39d5fc001e33197/numpy/core/src/multiarray/compiled_base.c#L1543). This is okay, we now have the tools to do this properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"fused notpacked popcount\" took 1251.4747999957763 nanoseconds per db row\n[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and_notpacked_count\n",
    "with Timer(\"fused notpacked popcount\", 1000000):\n",
    "    query_f = compute_fingerprint(query).astype(np.uint8)\n",
    "    fused_popcount_bitwise_and_notpacked_count(query_f, fingerprints[:1000000])\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endgame: AVX2\n",
    "AVX2 is supported by most modern processors, including the Intel processor that the specified MBP 2019 has.\n",
    "\n",
    "We get an absurdly high throughput: we're processing about 5 boolean database entries per nanosecond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"avx2\" took 116.35090000345372 nanoseconds per db row\n[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_avx2\n",
    "with Timer(\"avx2\", 1000000):\n",
    "    query_f = compute_fingerprint(query).astype(np.uint8)\n",
    "    scores = fused_popcount_avx2(query_f, fingerprints[:1000000]) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments and things to improve\n",
    "\n",
    "Algorithmic complexity is O(database size), since it must iterate over all the booleans in every row. I'm fairly certain that this cannot be asymptotically improved - for example in the worst case every molecule in the database contains all substructures (all 2048 bits are true).\n",
    "\n",
    "Memory usage is high; we're reading the entire database into memory and keeping it there. There isn't really any way around keeping O(database size) in memory because SSD bandwidth is far too low to support streaming from disk, but an 8x memory usage/bandwidth bottleneck improvement can probably be found by bitpacking the database beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Overtime stuff\n",
    "\n",
    "This was done for fun significantly over the 6hr time limit; please don't count this if you're evaluating quantity of code produced.\n",
    "\n",
    "## Pre-packing\n",
    "\n",
    "I did no memory-related optimizations mostly because I was more interested in playing with AVX, and because up until AVX the whole thing was compute-bottlenecked rather than memory-bottlenecked. However it's trivial to do so; just don't count the bitpacking step from the `Fusing operations` section, and you get (only!) a slight speedup over the AVX implementation as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"pre-packing process\" took 243.62619999737944 nanoseconds per db row\n\"pre-packed rows\" took 125.76629999966828 nanoseconds per db row\n[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount_bitwise_and\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "with Timer(\"pre-packed rows\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "    fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVX2 popcount attempt 1\n",
    "\n",
    "Not working correctly, but AVX2 emulated popcnt [can be 30% faster](https://stackoverflow.com/a/50082218) than the dedicated instruction.\n",
    "\n",
    "Edit: I think it won't be faster; according to [this paper](https://arxiv.org/pdf/1611.07612.pdf) and other sources, popcnt on 64-bit ints is the fastest way to do it at our data size of 2048 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"pre-packing process\" took 242.93290000059642 nanoseconds per db row\n\"pre-packed rows\" took 55.07950000173878 nanoseconds per db row\n[4.03833063e+15 2.62815452e+16 3.77945367e+15 3.34960946e+16\n 8.88579474e+15]\n[439306  28215  93292 489702 106223] [3.83505627e+16 3.83506231e+16 3.83505872e+16 3.83507498e+16\n 3.83504301e+16]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_avx2_emulated_popcount\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint64)\n",
    "    fused_avx2_emulated_popcount(query_packed, fingerprints_packed)\n",
    "    scores = np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcount64 and some tweaks to get good assembly\n",
    "\n",
    "Let's use popcount64 directly on 64-bit ints, instead of 32-bit ints, and also do counting in the kernel itself. (Also added loop unrolling flag, for the slight benefit of all these examples.)\n",
    "\n",
    "Maybe let's be less guessy. Here's a relevant assembly sample from a single iteration of the unrolled loop.\n",
    "```\n",
    "   27c1a:       44 09 c2                or     %r8d,%edx\n",
    "   27c1d:       41 89 07                mov    %eax,(%r15)\n",
    "   27c20:       48 63 ca                movslq %edx,%rcx\n",
    "   27c23:       49 0f af cc             imul   %r12,%rcx\n",
    "   27c27:       49 8b 14 0b             mov    (%r11,%rcx,1),%rdx\n",
    "   27c2b:       48 23 17                and    (%rdi),%rdx\n",
    "   27c2e:       31 c9                   xor    %ecx,%ecx\n",
    "   27c30:       4c 01 cf                add    %r9,%rdi\n",
    "   27c33:       f3 48 0f b8 ca          popcnt %rdx,%rcx\n",
    "   27c38:       8d 56 02                lea    0x2(%rsi),%edx\n",
    "   27c3b:       01 c8                   add    %ecx,%eax\n",
    "```\n",
    "There might be a tiny bit of gain to be had by avoiding some of these instructions. I don't fully understand what they do, though.\n",
    "\n",
    "Making `count` as a separate int variable (instead of accumulating directly into `counts[i>>5]`) results in much less consistent loop unrolling - there doesn't actually seem to be a particular pattern to the iterations; they have a bunch of random instructions squeezed in between that don't necessarily repeat. Finally, making `fingerprints_packed_curr` as a temporary variable instead of calculating `i|j` every time makes the loop much tighter:\n",
    "```\n",
    "   27d17:       4c 8b 79 d0             mov    -0x30(%rcx),%r15    ; Get a quadword of fingerprints_packed_curr (fixed offset)\n",
    "   27d1b:       4c 23 3a                and    (%rdx),%r15         ; AND it with a query quadword\n",
    "   27d1e:       48 01 f2                add    %rsi,%rdx           ; Advance the query pointer\n",
    "   27d21:       f3 4d 0f b8 ff          popcnt %r15,%r15           ; Popcount it\n",
    "   27d26:       41 01 c7                add    %eax,%r15d          ; Add this to the count\n",
    "```\n",
    "Those two modifications combined give an additional 25-30% better performance. I don't think there's anything else that can be done to speed this up, unless AVX2 has some real magic to offer (but even then, the frequency hit from AVX/AVX2 probably means that it won't be able to overcome this).\n",
    "\n",
    "The fastest I've seen for just the `fused_popcount64_bitwise_and` part is 30ns:\n",
    "- 2048bits/30ns = 8.5 GBps, out of a memory bandwidth of ~10-20GBps.\n",
    "- 32 popcounts / 30ns = approx one 64-bit popcount per 0.9 nanoseconds. This almost double the expected CPI of 5 cycles @ 3GHz = 1.6 ns, assuming the 5 listed instructions above take 1 cycle each, but apparently this is very superscalar, or my math is wrong (is frequency == cycles per second?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"pre-packing process\" took 204.6734000032302 nanoseconds per db row\n",
      "\"pre-packed rows\" took 26.365339999028947 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='little'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows\", 10000000):\n",
    "    for i in range(10):\n",
    "        query_f = compute_fingerprint(query)\n",
    "        query_packed = np.frombuffer(np.packbits(query_f, bitorder='little'), dtype=np.uint64)\n",
    "        counts = fused_popcount64_bitwise_and(query_packed, fingerprints_packed)\n",
    "        total = fused_popcount64_bitwise_and(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "        topk = np.argpartition(-counts, k)[:k]\n",
    "print(counts[:k]/total)\n",
    "print(topk, counts[topk]/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVX2 attempt 2: success and maximum throughput\n",
    "\n",
    "It might be possible to use AVX2 instructions to MOV or AND or ADD more efficiently; 10ns is the absolute limit because the popcount instruction has a CPI of 1. I'm highly doubtful it'll make things faster given that the width is only 256 bits anyway though, and there has to be some penalty moving between AVX registers and the regular registers where popcnt lives, though maybe that can be pipelined away.\n",
    "- LOADU twice for 256 bits each\n",
    "- AND 256 bits together\n",
    "- _mm256_extract_epi64 and popcount each of the 4 registers, adding\n",
    "\n",
    "With some guidance from [this](https://github.com/WojciechMula/sse-popcount/blob/master/popcnt-avx2-lookup.cpp) we can get a 20% speed boost.\n",
    "\n",
    "It turns out that even though benchmarks show unrolled loop of int64 popcnt is the fastest popcount implementation for 256 bytes, for our usecase we have additional math alongside it (the AND) so putting everything into AVX is actually beneficial.\n",
    "\n",
    "Our inner loop looks something like (\"something like\" because loop unrolling has made things confusing) this:\n",
    "```\n",
    "   28b23:       c4 43 25 38 6d 10 01    vinserti128 $0x1,0x10(%r13),%ymm11,%ymm13\n",
    "   28b2a:       c5 d5 71 d4 04          vpsrlw $0x4,%ymm4,%ymm5\n",
    "   28b2f:       c4 63 35 38 54 07 50    vinserti128 $0x1,0x50(%rdi,%rax,1),%ymm9,%ymm10\n",
    "   28b36:       01 \n",
    "   28b37:       c4 c1 7d fc c7          vpaddb %ymm15,%ymm0,%ymm0\n",
    "   28b3c:       c5 6d db c5             vpand  %ymm5,%ymm2,%ymm8\n",
    "   28b40:       c5 dd db f2             vpand  %ymm2,%ymm4,%ymm6\n",
    "   28b44:       c4 c1 15 db ca          vpand  %ymm10,%ymm13,%ymm1\n",
    "   28b49:       c4 42 1d 00 f0          vpshufb %ymm8,%ymm12,%ymm14\n",
    "   28b4e:       c4 41 7a 6f 14 24       vmovdqu (%r12),%xmm10\n",
    "   28b54:       c4 e2 1d 00 fe          vpshufb %ymm6,%ymm12,%ymm7\n",
    "   28b59:       c5 7a 6f 44 07 60       vmovdqu 0x60(%rdi,%rax,1),%xmm8\n",
    "```\n",
    "Even though it's longer, it's also handling 4x as much data per iteration.\n",
    "\n",
    "Turning on `-march=native -mno-avx256-split-unaligned-load` gets us to something like\n",
    "```\n",
    "   289ac:       c4 41 45 db cd          vpand  %ymm13,%ymm7,%ymm9\n",
    "   289b1:       c5 7d fc fe             vpaddb %ymm6,%ymm0,%ymm15\n",
    "   289b5:       c5 ed db bc 07 a0 00    vpand  0xa0(%rdi,%rax,1),%ymm2,%ymm7\n",
    "   289bc:       00 00 \n",
    "   289be:       c4 c1 05 fc f6          vpaddb %ymm14,%ymm15,%ymm6\n",
    "   289c3:       c4 e2 1d 00 e1          vpshufb %ymm1,%ymm12,%ymm4\n",
    "   289c8:       c4 c1 15 db c8          vpand  %ymm8,%ymm13,%ymm1\n",
    "   289cd:       c4 c2 1d 00 d9          vpshufb %ymm9,%ymm12,%ymm3\n",
    "   289d2:       c5 bd 71 d7 04          vpsrlw $0x4,%ymm7,%ymm8\n",
    "```\n",
    "Where we only see instructions that we've specified explicitly in the code. In fact, we're missing a bunch of `vmovdqu`'s - the compiler is doing something clever here that I don't understand.\n",
    "\n",
    "- 2048bits/22ns = 11.6 GBps, out of a memory bandwidth of ~10-20GBps.\n",
    "- ~80 AVX instructions / 22ns = approx 3.6GHz, which is significantly higher than the clock frequency. This is probably because some instructions can have a throughput of more than 1 per cycle.\n",
    "\n",
    "This may well be the end of the line for optimizing this, as this does seem to be optimal (all sources point to Mula as the source of these algorithms, and this is the best his papers have to offer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"pre-packing process\" took 208.82309999433346 nanoseconds per db row\n",
      "\"pre-packed rows 1\" took 0.8618699997896329 nanoseconds per db row\n",
      "\"pre-packed rows 2\" took 0.020319999021012336 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 19.185789999028202 nanoseconds per db row\n",
      "\"pre-packed rows 4\" took 0.004149999585933983 nanoseconds per db row\n",
      "\"pre-packed rows 5\" took 4.781700001331046 nanoseconds per db row\n",
      "[0.25155925 0.22245322 0.27858628 0.22453222 0.3035343 ]\n",
      "[329123 324807 324852 329172 324758] [0.66735967 0.66735967 0.66943867 0.66735967 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='little'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 1\", 10000000):\n",
    "    for i in range(10):\n",
    "        query_f = compute_fingerprint(query)\n",
    "with Timer(\"pre-packed rows 2\", 10000000):\n",
    "    for i in range(10):\n",
    "        query_packed = np.frombuffer(np.packbits(query_f, bitorder='little'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 3\", 10000000):\n",
    "    for i in range(10):\n",
    "        counts = fused_popcount64_bitwise_and_avx(query_packed, fingerprints_packed)\n",
    "with Timer(\"pre-packed rows 4\", 10000000):\n",
    "    for i in range(10):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "with Timer(\"pre-packed rows 5\", 10000000):\n",
    "    for i in range(10):\n",
    "        topk = np.argpartition(-counts, k)[:k]\n",
    "print(counts[:k]/total)\n",
    "print(topk, counts[topk]/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finale: Optimizing other parts of the system\n",
    "\n",
    "We're actually getting to the point where the final np.argpartition on 1 million elements is taking about 25% of the total time. Using np.argsort is 10x slower. We did actually need this initially seemingly unnecessary optimization!\n",
    "\n",
    "There are [ways](https://github.com/WojciechMula/simd-sort) to make this happen faster, but since we're going to be generating the entire count array anyway, why not just construct it on the fly? Specifically, let's use a **min-heap of size k** to track the k largest elements we've seen.\n",
    "\n",
    "This absorbs the topk algorithm without any measurable performance penalty (less than 10% with k=1000). A bit of tweaking suffices to double the speed of the compute_fingerprint, and now we are truly at the end of the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"pre-packing process\" took 236.88080000283662 nanoseconds per db row\n",
      "\"pre-packed rows 0\" took 0.14464399995631538 nanoseconds per db row\n",
      "\"pre-packed rows 1\" took 0.3155939999851398 nanoseconds per db row\n",
      "\"pre-packed rows 2\" took 0.030732000013813376 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.905900003621355 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.726300003938377 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.875900004175492 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 21.41499999561347 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 19.78449999296572 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.278400002396666 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.587199992500246 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.659399989293888 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.29240000329446 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.376499988837168 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.006199992261827 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.986900013056584 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.15779999701772 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.709999996237457 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.876800000318326 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.918599998462014 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.68380000430625 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.112099998281337 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.30269999441225 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.514000002644025 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.383099995437078 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.420999992988072 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.964499998721294 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.857099991990255 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.094299993710592 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.428600007202476 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.8781999964267 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.403899993747473 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.89390000340063 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.066999989561737 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.497699994943105 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.420100000686944 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.083800001069903 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.165299995918758 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.165299995918758 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.088100004708394 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.229100001510233 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.447199999471193 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.795899999327958 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.813099991646594 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.707500002463348 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.96669999707956 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.36530000541825 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.406900002039038 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.339500001980923 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.66710000426974 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.966499999398366 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.620499998680316 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.814400001545435 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.652599995722994 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.851400010637008 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.844399997149592 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.57799999904819 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.48220000101719 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 19.286900002043694 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.98789999843575 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.040100003010593 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.971400003763847 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.714700002921745 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.780900008394383 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.50779999908991 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.05740000377409 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.48820000991691 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.908200002741069 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.25989998865407 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.168800011859275 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.22530000167899 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.621700003976002 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.65059999888763 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.037699999287724 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.789600005722605 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.98849998845253 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.379599997890182 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.937299991492182 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.695500002242625 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.944899991154669 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.67319999251049 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.730500002973715 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.123500001616776 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.87750000564847 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.213599999900907 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.49649999651592 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.884199994616212 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.316400007577615 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 18.64130000467412 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.440999995917082 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.764700005296618 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.815499995369462 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.06049999827519 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.81879999139346 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.13409999117721 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.028199999709614 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.193499992368743 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.598500001942739 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.504800005350262 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.023199994582683 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.796600000816397 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 15.365700004622338 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 16.11850000335835 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 17.817400002968498 nanoseconds per db row\n",
      "\"pre-packed rows 4\" took 0.00258400003076531 nanoseconds per db row\n",
      "[324758 329123 324807 324852 329172] [0.66735967 0.66735967 0.66735967 0.66943867 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx_topk, fused_popcount64_bitwise_and_avx\n",
    "from rdkit import Chem\n",
    "\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='big'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 0\", 100000000):\n",
    "    for i in range(100):\n",
    "        mol = Chem.MolFromSmiles(query)\n",
    "with Timer(\"pre-packed rows 1\", 100000000):\n",
    "    for i in range(100):\n",
    "        s = Chem.RDKFingerprint(mol, fpSize=2048, maxPath=5).ToBitString()\n",
    "with Timer(\"pre-packed rows 2\", 100000000):\n",
    "    for i in range(100):\n",
    "        query_packed = np.copy(np.frombuffer(int(s, 2).to_bytes(len(s) // 8, byteorder='big'), dtype=np.uint64))\n",
    "for i in range(100):\n",
    "    with Timer(\"pre-packed rows 3\", 1000000):\n",
    "        counts, topk = fused_popcount64_bitwise_and_avx_topk(query_packed, fingerprints_packed, k)\n",
    "with Timer(\"pre-packed rows 4\", 100000000):\n",
    "    for i in range(100):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "print(topk, counts/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenMP Threading\n",
    "\n",
    "After some OpenMP we can smash the 20ns barrier.\n",
    "\n",
    "\"personal best\" = \"5.8700999999814485 nanoseconds per db row\" for `fused_popcount64_bitwise_and_avx_topk_omp`\n",
    "\n",
    "3600MHz RAM, 2048 bits / 5.869199999978036 ns = 43.6175288 GBps, after moving to my Ryzen 3700x machine which should have a max of 28.8GBps per stick, times two sticks.\n",
    "\n",
    "Going from 2133MHz to 3600MHz speeds up by 50%, and overclocking CPU doesn't visibly help, so it's legitimately memory bandwidth bound, and even after the frequency increase to 3600MHz it's still probably memory bound. That's pretty wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"pre-packing process\" took 231.52609998942353 nanoseconds per db row\n",
      "\"pre-packed rows 0\" took 0.1494789999560453 nanoseconds per db row\n",
      "\"pre-packed rows 1\" took 0.3410159998747986 nanoseconds per db row\n",
      "\"pre-packed rows 2\" took 0.029672999953618277 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 12.801200000103563 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.323259999975562 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.880690000252798 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.2036400000797585 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.760049999866169 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.0444299999508075 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.81685000017751 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.184160000178963 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.467060000635684 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.2605199995450675 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.1634799998719245 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.244459999492392 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.224320000386797 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.073800000944175 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.121129998995457 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.43361999921035 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 8.111089999147225 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.4623099993914375 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.471489999443293 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.512259999930393 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.444669998949394 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.541439999477006 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.445889999507926 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.117299998935778 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.128899999952409 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.169399999838788 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.934450000699144 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.850890000350773 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.050640000670683 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.938859999354463 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.949809999787249 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.840689999808092 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.25836000056006 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.866160000208765 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.895139999687672 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.23400999995647 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.883160000143107 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.984150000789668 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.07098999992013 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.379529999161605 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.792369999515358 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.337380001263227 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.397460000356659 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.309819999500178 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.292669999878854 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.768269999884069 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.126520000747405 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.09084999980405 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 7.119209998927545 nanoseconds per db row\n",
      "\"pre-packed rows 3\" took 6.9831800006795675 nanoseconds per db row\n",
      "\"pre-packed rows 4\" took 0.0029880000511184335 nanoseconds per db row\n",
      "[324758 324807 329123 324852 329172] [0.66735967 0.66735967 0.66735967 0.66943867 0.66735967]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx_topk_omp, fused_popcount64_bitwise_and_avx\n",
    "from rdkit import Chem\n",
    "\n",
    "import os\n",
    "# This only works before the first run of openmp in a process\n",
    "os.environ[\"OMP_PLACES\"] = \"cores\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='big'), dtype=np.uint64)\n",
    "with Timer(\"pre-packed rows 0\", 100000000):\n",
    "    for i in range(100):\n",
    "        mol = Chem.MolFromSmiles(query)\n",
    "with Timer(\"pre-packed rows 1\", 100000000):\n",
    "    for i in range(100):\n",
    "        s = Chem.RDKFingerprint(mol, fpSize=2048, maxPath=5).ToBitString()\n",
    "with Timer(\"pre-packed rows 2\", 100000000):\n",
    "    for i in range(100):\n",
    "        query_packed = np.copy(np.frombuffer(int(s, 2).to_bytes(len(s) // 8, byteorder='big'), dtype=np.uint64))\n",
    "for i in range(50):\n",
    "    with Timer(\"pre-packed rows 3\", 10000000):\n",
    "        for i in range(10):\n",
    "            counts, topk = fused_popcount64_bitwise_and_avx_topk_omp(query_packed, fingerprints_packed, k)\n",
    "with Timer(\"pre-packed rows 4\", 100000000):\n",
    "    for i in range(100):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "print(topk, counts/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed: blosc\n",
    "Since we might have a memory bottleneck, does blosc help?\n",
    "\n",
    "Nope, that was a total failure. Maybe next step is to try out custom encodings like RLE or even sparse bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Compression: 0.70317598828125\n"
     ]
    }
   ],
   "source": [
    "import blosc\n",
    "fingerprints_blosc_compressed = np.frombuffer(blosc.compress(fingerprints_packed, typesize=8, shuffle=blosc.NOSHUFFLE, cname='blosclz'), dtype=np.uint8)\n",
    "\n",
    "print(\"Compression:\", len(fingerprints_blosc_compressed)/8/len(fingerprints_packed))\n",
    "# %timeit blosc.decompress(fingerprints_blosc_compressed)\n",
    "# %timeit np.copy(fingerprints_packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"pre-packing process\" took 547.3415000014938 nanoseconds per db row\n\"pre-packed rows 0\" took 0.2928000030806288 nanoseconds per db row\n\"pre-packed rows 1\" took 0.332200012053363 nanoseconds per db row\n\"pre-packed rows 2\" took 0.04569999873638153 nanoseconds per db row\n\"pre-packed rows 3\" took 56.81229999754578 nanoseconds per db row\n\"pre-packed rows 4\" took 0.015300000086426733 nanoseconds per db row\n[ 999436  999437 1003531 1003532 1003533] [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from popcount import fused_popcount64_bitwise_and_avx_topk_omp_blosc, fused_popcount64_bitwise_and_avx\n",
    "from rdkit import Chem\n",
    "import blosc\n",
    "\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints, axis=1, bitorder='big'), dtype=np.uint64)\n",
    "    fingerprints_blosc_compressed = np.frombuffer(blosc.compress(fingerprints_packed, typesize=8, shuffle=blosc.NOSHUFFLE, cname='blosclz'), dtype=np.uint8)\n",
    "    n_fingerprints = len(fingerprints)\n",
    "with Timer(\"pre-packed rows 0\", 1000000):\n",
    "    for i in range(1):\n",
    "        mol = Chem.MolFromSmiles(query)\n",
    "with Timer(\"pre-packed rows 1\", 1000000):\n",
    "    for i in range(1):\n",
    "        s = Chem.RDKFingerprint(mol, fpSize=2048, maxPath=5).ToBitString()\n",
    "with Timer(\"pre-packed rows 2\", 1000000):\n",
    "    for i in range(1):\n",
    "        query_packed = np.copy(np.frombuffer(int(s, 2).to_bytes(len(s) // 8, byteorder='big'), dtype=np.uint64))\n",
    "with Timer(\"pre-packed rows 3\", 1000000):\n",
    "    for i in range(1):\n",
    "        counts, topk = fused_popcount64_bitwise_and_avx_topk_omp_blosc(query_packed, fingerprints_blosc_compressed, k, n_fingerprints)\n",
    "with Timer(\"pre-packed rows 4\", 1000000):\n",
    "    for i in range(1):\n",
    "        total = fused_popcount64_bitwise_and_avx(query_packed, query_packed) # About twice as fast as np.sum(query_packed)\n",
    "print(topk, counts/total)"
   ]
  },
  {
   "source": [
    "## Failed: Sparsity\n",
    "\n",
    "We can see that the database is somewhat sparse. Is this enough to be useful?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.154494466796875 cell sparsity\n0.23486328125 column sparsity\n"
     ]
    }
   ],
   "source": [
    "print(fingerprints.sum() / fingerprints.shape[0] / fingerprints.shape[1], \"cell sparsity\")\n",
    "print(compute_fingerprint(query).sum() / fingerprints.shape[1], \"column sparsity\")"
   ]
  },
  {
   "source": [
    "In principle we can access only 0.154494466796875 * 0.23486328125 = 0.0362850774 of the database.\n",
    "However each item is no longer 1 bit - it must be at least 32 bits to store the location.\n",
    "But perhaps less if we're clever about encoding.\n",
    "So... 'tis dubious whether sparsity can possibly help."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from scipy import sparse\n",
    "fingerprints_T = sparse.csr_matrix(fingerprints.T, dtype=np.uint16)\n",
    "fingerprints_T.count_nonzero()\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "316404668"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"scipy sparse: fingerprint\" took 1.013399989460595 nanoseconds per db row\n",
      "\"scipy sparse: boolean slice\" took 240.63740001292902 nanoseconds per db row\n",
      "\"scipy sparse: sum\" took 610.1626000017859 nanoseconds per db row\n",
      "\"scipy sparse: argpartition\" took 5.4133999947225675 nanoseconds per db row\n",
      "[[329123 324807 324852 329172 324758]]\n"
     ]
    }
   ],
   "source": [
    "k=5\n",
    "with Timer(\"scipy sparse: fingerprint\", 1000000):\n",
    "    query_bool = np.asarray(compute_fingerprint(query), dtype=np.bool)\n",
    "with Timer(\"scipy sparse: boolean slice\", 1000000):\n",
    "    slice = fingerprints_T[query_bool]\n",
    "with Timer(\"scipy sparse: sum\", 1000000):\n",
    "    scores = slice.sum(axis=0)\n",
    "with Timer(\"scipy sparse: argpartition\", 1000000):\n",
    "    # scores = np.sum(fingerprints_T[np.asarray(compute_fingerprint(query), dtype=np.bool)], axis=0)\n",
    "    topk = np.argpartition(-scores, k)[:,:k]\n",
    "print(topk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[710]\n(183009, 1)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "316404668"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "v = fingerprints.T[100]\n",
    "a = np.argwhere(v)\n",
    "print(max(a[1:] - a[:-1])) # runlength compression or something\n",
    "# print(fingerprints.T[:,1:] - fingerprints.T[:,:-1])\n",
    "# print(a)\n",
    "print(a.shape)\n",
    "fingerprints.sum()\n",
    "# Note that we can reorder the database however we want as long as we translate back the results afterward.\n",
    "# Maybe given the compression performance (only like 30% size reduction) we actually legitimately cannot do anything here."
   ]
  },
  {
   "source": [
    "## Is there truly nothing more we can do on CPU?\n",
    "\n",
    "We're at something like 75% memory bandwidth. Is this really the limit?\n",
    "\n",
    "AVX-512 would be fun but I don't have ice lake hardware sadly. This should be something like 3x faster, less any AVX-induced processor frequency hit.\n",
    "- `_mm512_loadu_epi64`\n",
    "- `_mm512_and_epi64`\n",
    "- `_mm512_popcnt_epi64`\n",
    "- `_mm512_reduce_add_epi64`\n",
    "\n",
    "Ryzen latency/throughput numbers: https://www.agner.org/optimize/instruction_tables.pdf\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The real endgame: CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pycuda in /home/clive/.local/lib/python3.7/site-packages (2020.1)\nRequirement already satisfied: pytools>=2011.2 in /home/clive/.local/lib/python3.7/site-packages (from pycuda) (2020.4)\nRequirement already satisfied: decorator>=3.2.0 in /home/clive/.local/lib/python3.7/site-packages (from pycuda) (4.4.2)\nRequirement already satisfied: appdirs>=1.4.0 in /home/clive/.local/lib/python3.7/site-packages (from pycuda) (1.4.4)\nRequirement already satisfied: mako in /home/clive/.local/lib/python3.7/site-packages (from pycuda) (1.1.3)\nRequirement already satisfied: six>=1.8.0 in /home/clive/.local/lib/python3.7/site-packages (from pytools>=2011.2->pycuda) (1.15.0)\nRequirement already satisfied: numpy>=1.6.0 in /home/clive/.local/lib/python3.7/site-packages (from pytools>=2011.2->pycuda) (1.19.2)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /home/clive/.local/lib/python3.7/site-packages (from mako->pycuda) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda.compiler import SourceModule\n",
    "from pycuda.gpuarray import GPUArray, to_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(32000000,)\n\"pre-packing process\" took 299.99749999842606 nanoseconds per db row\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed_gpu = to_gpu(np.frombuffer(np.packbits(fingerprints, axis=1, bitorder='big'), dtype=np.uint64))\n",
    "    print(fingerprints_packed_gpu.shape)\n",
    "    sums_gpu = GPUArray((1000*k,), dtype=np.int32)\n",
    "    indexes_gpu = GPUArray((1000*k,), dtype=np.int32)\n",
    "    n_fingerprints = len(fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SourceModule(f\"#define k {k}\" + \"\"\"\n",
    "  __global__ void and_popcount(unsigned long long *a, unsigned long long *b, int *sums, int *indexes)\n",
    "  {\n",
    "    int *my_sums = &sums[(blockDim.x * blockIdx.x + threadIdx.x) * k];\n",
    "    int *my_indexes = &indexes[(blockDim.x * blockIdx.x + threadIdx.x) * k];\n",
    "    for(int i = 0; i < k; i++) {\n",
    "       my_sums[i] = 0;\n",
    "    }\n",
    "    int START = (blockIdx.x * blockDim.x + threadIdx.x) * 1000;\n",
    "    int END = min(START + 1000, 1000000);\n",
    "    for (int i = START; i < END; i++) {\n",
    "      int sum = 0;\n",
    "      unsigned long long *a_tmp = &a[i<<5];\n",
    "      for (int j = 0; j < 32; j++) {\n",
    "        sum += __popcll(a_tmp[j] & b[j]);\n",
    "      }\n",
    "      int index = i;\n",
    "      for(int i = 0; i < k; i++) {\n",
    "        if(my_sums[i] < sum) {\n",
    "          int tmp = my_sums[i];\n",
    "          my_sums[i] = sum;\n",
    "          sum = tmp;\n",
    "          int tmp2 = my_indexes[i];\n",
    "          my_indexes[i] = index;\n",
    "          index = tmp2;\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \"\"\", options=['--use_fast_math', '-O3', '-Xptxas', '-O3,-v'])\n",
    "and_popcount = mod.get_function(\"and_popcount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"cuda: mol\" took 0.13096130000485573 nanoseconds per db row\n",
      "\"cuda: fingerprint\" took 0.3019350999966264 nanoseconds per db row\n",
      "\"cuda: and_popcount\" took 3.0348902999976417 nanoseconds per db row\n",
      "\"cuda: pull top 5\" took 0.014718600010382945 nanoseconds per db row\n",
      "[324852 324758 329172 324807 329123]\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "with Timer(\"cuda: mol\", 1000000000):\n",
    "    for _ in range(1000):\n",
    "        mol = Chem.MolFromSmiles(query)\n",
    "with Timer(\"cuda: fingerprint\", 1000000000):\n",
    "    for _ in range(1000):\n",
    "        s = Chem.RDKFingerprint(mol, fpSize=2048, maxPath=5).ToBitString()\n",
    "        query_packed = np.frombuffer(int(s, 2).to_bytes(len(s) // 8, byteorder='big'), dtype=np.uint64)\n",
    "with Timer(\"cuda: and_popcount\", 1000000000):\n",
    "    for _ in range(1000):\n",
    "        query_packed_gpu = to_gpu(query_packed)\n",
    "        and_popcount(fingerprints_packed_gpu, query_packed_gpu, sums_gpu, indexes_gpu, grid=(32,1,1), block=(32,1,1))\n",
    "        indexes = indexes_gpu.get()\n",
    "        sums = sums_gpu.get()\n",
    "with Timer(\"cuda: pull top 5\", 1000000000):\n",
    "    for _ in range(1000):\n",
    "        topk = indexes[np.argpartition(-sums, k)[:k]]\n",
    "print(topk)\n",
    "# np.argpartition(-sums_gpu, k)[:k]"
   ]
  },
  {
   "source": [
    "## NEXT STEPS FOR CUDA\n",
    "\n",
    "https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Threading\n",
    "Because why not? The AVX2 code is mostly in C-land away from the GIL so we might be able to get close to linear speedup in the number of cores (regular cores; hyperthreading probably does not help for this because AVX2 resources are shared).\n",
    "\n",
    "This didn't help at all with the AVX2 code because there are other bottlenecks e.g. system memory bandwidth. 2GB dataset in 0.2s is 10GBps, which is around half the theoretical memory bandwidth my machine could have, but there's probably some other caveats there.\n",
    "\n",
    "This does help a lot (+50% ish) with regular popcount64 because of the 8x lower memory bandwidth requirement when bits are packed. 2GB/8 = 256MB dataset in 0.08s is only 3GBps, so it's probably still compute limited and could benefit from more threading. Unfortunately I only have 2 actual cores (4 hyperthreaded) so there can be no more than a 2x speedup.\n",
    "\n",
    "However modifying popcount64 to fuse the counting step as well results in having no significant threading benefit. Occasionally it will run faster than single-threaded, but most of the time it's running into some issue or another - maybe memory bandwidth, maybe scheduler issues because it's so fast.\n",
    "\n",
    "This also points toward there being very little point in using OpenCL: unless you have a discrete GPU with dedicated high bandwidth video memory, which isn't the case on MBP 2019, it's not going to get you much further, and more likely will slow things down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from popcount import fused_popcount64_bitwise_and\n",
    "with Timer(\"pre-packing process\", 1000000):\n",
    "    fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1, bitorder='little'), dtype=np.uint64)\n",
    "\n",
    "core_ids = set()\n",
    "with open('/proc/cpuinfo') as f:\n",
    "    for line in f:\n",
    "        if line.count('core id'):\n",
    "            core_ids.add(line)\n",
    "\n",
    "nthreads = len(core_ids)\n",
    "assert(len(fingerprints) % nthreads == 0) # Note: below code calculating start and end assumes that nthreads divides len(fingerprints)\n",
    "print(f\"Using {nthreads} threads\")\n",
    "\n",
    "topk_per_thread = [None] * nthreads\n",
    "topk_per_thread_scores = [None] * nthreads\n",
    "\n",
    "def thread_func(query_packed, threadid):\n",
    "    start = threadid*len(fingerprints_packed)//nthreads\n",
    "    end = (threadid+1)*len(fingerprints_packed)//nthreads\n",
    "    \n",
    "    counts = fused_popcount64_bitwise_and(query_packed, fingerprints_packed[start:end])\n",
    "    total = fused_popcount64_bitwise_and(query_packed, query_packed)\n",
    "\n",
    "    topk_per_thread[threadid] = np.argpartition(-counts, k)[:k]\n",
    "\n",
    "    topk_per_thread_scores[threadid] = counts[topk_per_thread[threadid]] / total\n",
    "    topk_per_thread[threadid] += start // 32\n",
    "\n",
    "with Timer(\"threaded popcount64\", 1000000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    query_packed = np.frombuffer(np.packbits(query_f, bitorder='little'), dtype=np.uint64)\n",
    "    threads = [None] * nthreads\n",
    "    for i in range(1, nthreads):\n",
    "        threads[i] = threading.Thread(target=thread_func, args=(query_packed, i))\n",
    "        threads[i].start()\n",
    "    thread_func(query_packed, 0)\n",
    "    for i in range(1, nthreads):\n",
    "        threads[i].join()\n",
    "    topk_arg = np.argsort(-np.concatenate(topk_per_thread_scores))[:k]\n",
    "    scores = np.concatenate(topk_per_thread_scores)[topk_arg]\n",
    "    topk = np.concatenate(topk_per_thread)[topk_arg]\n",
    "print(topk, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: A failed attempt at using Numba\n",
    "Cython has some ... interoperability issues ... with Numba, so using the popcount kernel isn't possible.\n",
    "\n",
    "Using purely Numba (without the popcount kernel) is way slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from numba import jit\n",
    "except ImportError:\n",
    "    ! conda install -y numba\n",
    "    from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "# from numba.extending import get_cython_function_address\n",
    "# import numpy.ctypeslib as npct\n",
    "# import ctypes\n",
    "\n",
    "# array_1d_uint32 = npct.ndpointer(dtype=np.uint32, ndim=1, flags='CONTIGUOUS')\n",
    "# addr = get_cython_function_address(\"popcount\", \"_fused_popcount_bitwise_and\")\n",
    "# functype = ctypes.CFUNCTYPE(None, array_1d_uint32, array_1d_uint32)\n",
    "# fused_popcount_bitwise_and = functype(addr)\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def numba_func(query_f, fingerprints):\n",
    "    return np.sum(query_f & fingerprints[:100000], axis=1) / np.sum(query_f)\n",
    "\n",
    "# @numba.jit(nopython=True)\n",
    "# def numba_func(query_f, fingerprints):\n",
    "#     query_packed = np.frombuffer(np.packbits(query_f), dtype=np.uint32)\n",
    "#     fingerprints_packed = np.frombuffer(np.packbits(fingerprints[:1000000], axis=1), dtype=np.uint32)\n",
    "#     fused_popcount_bitwise_and(query_packed, fingerprints_packed)\n",
    "#     return np.sum(fingerprints_packed.reshape(-1, 2048//32), axis=1) / np.sum(query_f)\n",
    "\n",
    "with Timer(\"numba\", 100000):\n",
    "    query_f = compute_fingerprint(query)\n",
    "    scores = numba_func(query_f, fingerprints)\n",
    "    topk = np.argpartition(-scores, k)[:k]\n",
    "print(scores[:k])\n",
    "print(topk, scores[topk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import popcount\n",
    "popcount = importlib.reload(popcount)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('my-rdkit-env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "98458a16397abcd6d406b39f5e7a767fa1036f778a763cf4e86be6b7e39a91e7"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}